* 
* ==> Audit <==
* |--------------|---------------------------|----------|------|---------|---------------------|---------------------|
|   Command    |           Args            | Profile  | User | Version |     Start Time      |      End Time       |
|--------------|---------------------------|----------|------|---------|---------------------|---------------------|
| image        | ls                        | minikube | eli  | v1.31.2 | 03 Nov 23 07:02 -03 | 03 Nov 23 07:02 -03 |
| addons       | enable ingress            | minikube | eli  | v1.31.2 | 03 Nov 23 07:37 -03 | 03 Nov 23 07:38 -03 |
| update-check |                           | minikube | eli  | v1.31.2 | 03 Nov 23 23:44 -03 | 03 Nov 23 23:44 -03 |
| start        |                           | minikube | eli  | v1.31.2 | 03 Nov 23 23:46 -03 | 03 Nov 23 23:46 -03 |
| addons       | enable ingress            | minikube | eli  | v1.31.2 | 04 Nov 23 00:03 -03 | 04 Nov 23 00:03 -03 |
| service      | web --url                 | minikube | eli  | v1.31.2 | 04 Nov 23 00:06 -03 | 04 Nov 23 00:06 -03 |
| service      | web --url                 | minikube | eli  | v1.31.2 | 04 Nov 23 00:12 -03 | 04 Nov 23 00:12 -03 |
| service      | translatorapi --url       | minikube | eli  | v1.31.2 | 04 Nov 23 00:16 -03 | 04 Nov 23 00:16 -03 |
| service      | translatorapi --url       | minikube | eli  | v1.31.2 | 04 Nov 23 00:40 -03 | 04 Nov 23 00:40 -03 |
| podman-env   |                           | minikube | eli  | v1.31.2 | 04 Nov 23 00:51 -03 |                     |
| stop         |                           | minikube | eli  | v1.31.2 | 04 Nov 23 00:52 -03 | 04 Nov 23 00:52 -03 |
| start        |                           | minikube | eli  | v1.31.2 | 04 Nov 23 00:52 -03 | 04 Nov 23 00:52 -03 |
| podman-env   |                           | minikube | eli  | v1.31.2 | 04 Nov 23 00:54 -03 |                     |
| stop         |                           | minikube | eli  | v1.31.2 | 04 Nov 23 00:55 -03 | 04 Nov 23 00:55 -03 |
| start        | --driver=podman           | minikube | eli  | v1.31.2 | 04 Nov 23 00:55 -03 |                     |
| stop         |                           | minikube | eli  | v1.31.2 | 04 Nov 23 00:56 -03 | 04 Nov 23 00:56 -03 |
| start        | --container-runtime=cri-o | minikube | eli  | v1.31.2 | 04 Nov 23 00:56 -03 |                     |
| start        | --container-runtime=cri-o | minikube | eli  | v1.31.2 | 04 Nov 23 00:57 -03 | 04 Nov 23 00:57 -03 |
| podman-env   |                           | minikube | eli  | v1.31.2 | 04 Nov 23 00:58 -03 | 04 Nov 23 00:58 -03 |
| service      | translatorapi --url       | minikube | eli  | v1.31.2 | 04 Nov 23 02:06 -03 |                     |
| stop         |                           | minikube | eli  | v1.31.2 | 04 Nov 23 02:10 -03 | 04 Nov 23 02:10 -03 |
| start        | --container-runtime=cri-o | minikube | eli  | v1.31.2 | 04 Nov 23 02:11 -03 | 04 Nov 23 02:12 -03 |
| stop         |                           | minikube | eli  | v1.31.2 | 04 Nov 23 02:15 -03 | 04 Nov 23 02:15 -03 |
| delete       |                           | minikube | eli  | v1.31.2 | 04 Nov 23 02:15 -03 | 04 Nov 23 02:15 -03 |
| start        | --container-runtime=cri-o | minikube | eli  | v1.31.2 | 04 Nov 23 02:19 -03 | 04 Nov 23 02:19 -03 |
| start        | --container-runtime=cri-o | minikube | eli  | v1.31.2 | 04 Nov 23 02:41 -03 | 04 Nov 23 02:41 -03 |
| podman-env   |                           | minikube | eli  | v1.31.2 | 04 Nov 23 02:41 -03 | 04 Nov 23 02:41 -03 |
| podman-env   |                           | minikube | eli  | v1.31.2 | 04 Nov 23 02:49 -03 | 04 Nov 23 02:49 -03 |
| stop         |                           | minikube | eli  | v1.31.2 | 04 Nov 23 02:55 -03 | 04 Nov 23 02:55 -03 |
| delete       |                           | minikube | eli  | v1.31.2 | 04 Nov 23 02:56 -03 | 04 Nov 23 02:56 -03 |
| start        |                           | minikube | eli  | v1.31.2 | 04 Nov 23 02:57 -03 | 04 Nov 23 02:58 -03 |
| docker-env   |                           | minikube | eli  | v1.31.2 | 04 Nov 23 02:58 -03 | 04 Nov 23 02:58 -03 |
| service      | translatorapi --url       | minikube | eli  | v1.31.2 | 04 Nov 23 03:03 -03 |                     |
| service      | translatorapi --url       | minikube | eli  | v1.31.2 | 04 Nov 23 03:04 -03 | 04 Nov 23 03:04 -03 |
| start        |                           | minikube | eli  | v1.31.2 | 09 Nov 23 07:41 -03 | 09 Nov 23 07:43 -03 |
| service      | simplerisk --url          | minikube | eli  | v1.31.2 | 09 Nov 23 08:12 -03 | 09 Nov 23 08:12 -03 |
| service      | simplerisk --url          | minikube | eli  | v1.31.2 | 09 Nov 23 08:33 -03 | 09 Nov 23 08:33 -03 |
| update-check |                           | minikube | eli  | v1.31.2 | 09 Nov 23 08:53 -03 | 09 Nov 23 08:53 -03 |
| addons       | enable ingress            | minikube | eli  | v1.31.2 | 09 Nov 23 09:03 -03 | 09 Nov 23 09:04 -03 |
| start        |                           | minikube | eli  | v1.31.2 | 12 Nov 23 18:19 -03 |                     |
| delete       |                           | minikube | eli  | v1.31.2 | 12 Nov 23 18:19 -03 | 12 Nov 23 18:19 -03 |
| start        |                           | minikube | eli  | v1.31.2 | 12 Nov 23 18:19 -03 |                     |
| start        |                           | minikube | eli  | v1.31.2 | 12 Nov 23 18:24 -03 | 12 Nov 23 18:25 -03 |
| service      | my-redmine --url          | minikube | eli  | v1.31.2 | 12 Nov 23 18:36 -03 | 12 Nov 23 18:36 -03 |
| start        |                           | minikube | eli  | v1.31.2 | 15 Nov 23 21:14 -03 | 15 Nov 23 21:15 -03 |
| service      | simplerisk-service --url  | minikube | eli  | v1.31.2 | 15 Nov 23 21:22 -03 | 15 Nov 23 21:22 -03 |
| service      | simplerisk-service --url  | minikube | eli  | v1.31.2 | 15 Nov 23 21:59 -03 | 15 Nov 23 21:59 -03 |
| service      | simplerisk-service --url  | minikube | eli  | v1.31.2 | 15 Nov 23 22:06 -03 | 15 Nov 23 22:06 -03 |
| start        |                           | minikube | eli  | v1.31.2 | 20 Nov 23 13:31 -03 | 20 Nov 23 13:32 -03 |
| stop         |                           | minikube | eli  | v1.31.2 | 21 Nov 23 07:16 -03 | 21 Nov 23 07:16 -03 |
| stop         |                           | minikube | eli  | v1.31.2 | 21 Nov 23 07:16 -03 | 21 Nov 23 07:16 -03 |
| stop         |                           | minikube | eli  | v1.31.2 | 21 Nov 23 07:16 -03 | 21 Nov 23 07:16 -03 |
| update-check |                           | minikube | eli  | v1.32.0 | 13 Jan 24 21:56 -03 | 13 Jan 24 21:56 -03 |
| start        |                           | minikube | eli  | v1.31.2 | 28 Jan 24 17:02 -03 | 28 Jan 24 17:03 -03 |
| update-check |                           | minikube | eli  | v1.32.0 | 28 Jan 24 17:04 -03 | 28 Jan 24 17:04 -03 |
| ip           |                           | minikube | eli  | v1.31.2 | 28 Jan 24 17:05 -03 | 28 Jan 24 17:05 -03 |
| docker-env   |                           | minikube | eli  | v1.31.2 | 28 Jan 24 17:20 -03 | 28 Jan 24 17:20 -03 |
| start        |                           | minikube | eli  | v1.31.2 | 28 Jan 24 17:37 -03 | 28 Jan 24 17:38 -03 |
| docker-env   |                           | minikube | eli  | v1.31.2 | 28 Jan 24 17:38 -03 | 28 Jan 24 17:38 -03 |
| service      | ner-extractor-svc --url   | minikube | eli  | v1.31.2 | 28 Jan 24 17:41 -03 |                     |
|--------------|---------------------------|----------|------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/01/28 17:37:52
Running on machine: machinetek
Binary: Built with gc go1.20.7 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0128 17:37:52.198929  158148 out.go:296] Setting OutFile to fd 1 ...
I0128 17:37:52.199045  158148 out.go:348] isatty.IsTerminal(1) = true
I0128 17:37:52.199048  158148 out.go:309] Setting ErrFile to fd 2...
I0128 17:37:52.199055  158148 out.go:348] isatty.IsTerminal(2) = true
I0128 17:37:52.199266  158148 root.go:338] Updating PATH: /home/eli/.minikube/bin
I0128 17:37:52.199287  158148 oci.go:573] shell is pointing to dockerd inside minikube. will unset to use host
I0128 17:37:52.199801  158148 out.go:303] Setting JSON to false
I0128 17:37:52.213399  158148 start.go:128] hostinfo: {"hostname":"machinetek","uptime":20702,"bootTime":1706453570,"procs":419,"os":"linux","platform":"debian","platformFamily":"debian","platformVersion":"12.4","kernelVersion":"6.1.0-16-amd64","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"cfe0d8e7-209e-4f3e-9a22-714b98f7a06f"}
I0128 17:37:52.213482  158148 start.go:138] virtualization: kvm host
I0128 17:37:52.214984  158148 out.go:177] üòÑ  minikube v1.31.2 on Debian 12.4
I0128 17:37:52.216642  158148 notify.go:220] Checking for updates...
I0128 17:37:52.216659  158148 out.go:177]     ‚ñ™ MINIKUBE_ACTIVE_DOCKERD=minikube
I0128 17:37:52.220649  158148 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0128 17:37:52.220738  158148 driver.go:373] Setting default libvirt URI to qemu:///system
I0128 17:37:52.247987  158148 docker.go:121] docker version: linux-24.0.7:Docker Engine - Community
I0128 17:37:52.248121  158148 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0128 17:37:52.315391  158148 info.go:266] docker info: {ID:6947141b-3bbb-453b-ac81-1a9f19effb65 Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:45 OomKillDisable:false NGoroutines:163 SystemTime:2024-01-28 17:37:52.303303708 -0300 -03 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.1.0-16-amd64 OperatingSystem:Debian GNU/Linux 12 (bookworm) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:33458995200 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:machinetek Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:pii8tagsc7dyp8qqm6qbh22jv NodeAddr:192.168.0.89 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.0.89:2377 NodeID:pii8tagsc7dyp8qqm6qbh22jv]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dd1e886e55dd695541fdcd67420c2888645a495 Expected:3dd1e886e55dd695541fdcd67420c2888645a495} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.21.0]] Warnings:<nil>}}
I0128 17:37:52.315794  158148 docker.go:294] overlay module found
I0128 17:37:52.316699  158148 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0128 17:37:52.318455  158148 start.go:298] selected driver: docker
I0128 17:37:52.318463  158148 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:7900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/eli:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0128 17:37:52.318541  158148 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0128 17:37:52.318641  158148 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0128 17:37:52.383654  158148 info.go:266] docker info: {ID:6947141b-3bbb-453b-ac81-1a9f19effb65 Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:5 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:45 OomKillDisable:false NGoroutines:163 SystemTime:2024-01-28 17:37:52.372469545 -0300 -03 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.1.0-16-amd64 OperatingSystem:Debian GNU/Linux 12 (bookworm) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:33458995200 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:machinetek Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:pii8tagsc7dyp8qqm6qbh22jv NodeAddr:192.168.0.89 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.0.89:2377 NodeID:pii8tagsc7dyp8qqm6qbh22jv]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dd1e886e55dd695541fdcd67420c2888645a495 Expected:3dd1e886e55dd695541fdcd67420c2888645a495} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.21.0]] Warnings:<nil>}}
I0128 17:37:52.384965  158148 cni.go:84] Creating CNI manager for ""
I0128 17:37:52.384978  158148 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0128 17:37:52.384994  158148 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:7900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/eli:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0128 17:37:52.386852  158148 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0128 17:37:52.387976  158148 cache.go:122] Beginning downloading kic base image for docker with docker
I0128 17:37:52.389124  158148 out.go:177] üöú  Pulling base image ...
I0128 17:37:52.390699  158148 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0128 17:37:52.390728  158148 preload.go:148] Found local preload: /home/eli/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I0128 17:37:52.390743  158148 cache.go:57] Caching tarball of preloaded images
I0128 17:37:52.390814  158148 preload.go:174] Found /home/eli/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0128 17:37:52.390823  158148 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I0128 17:37:52.390825  158148 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I0128 17:37:52.390909  158148 profile.go:148] Saving config to /home/eli/.minikube/profiles/minikube/config.json ...
I0128 17:37:52.412657  158148 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon, skipping pull
I0128 17:37:52.412681  158148 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in daemon, skipping load
I0128 17:37:52.412712  158148 cache.go:195] Successfully downloaded all kic artifacts
I0128 17:37:52.412742  158148 start.go:365] acquiring machines lock for minikube: {Name:mkbcb13d253a86f826e15308f384ba60ca6c8f96 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0128 17:37:52.412823  158148 start.go:369] acquired machines lock for "minikube" in 43.106¬µs
I0128 17:37:52.412839  158148 start.go:96] Skipping create...Using existing machine configuration
I0128 17:37:52.412844  158148 fix.go:54] fixHost starting: 
I0128 17:37:52.414410  158148 out.go:177] üìå  Noticed you have an activated docker-env on docker driver in this terminal:
W0128 17:37:52.415709  158148 out.go:239] ‚ùó  Please re-eval your docker-env, To ensure your environment variables have updated ports:

	'minikube -p minikube docker-env'

	
I0128 17:37:52.415791  158148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 17:37:52.436143  158148 fix.go:102] recreateIfNeeded on minikube: state=Running err=<nil>
W0128 17:37:52.436174  158148 fix.go:128] unexpected machine state, will restart: <nil>
I0128 17:37:52.437325  158148 out.go:177] üèÉ  Updating the running docker "minikube" container ...
I0128 17:37:52.439055  158148 machine.go:88] provisioning docker machine ...
I0128 17:37:52.439073  158148 ubuntu.go:169] provisioning hostname "minikube"
I0128 17:37:52.439136  158148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 17:37:52.462148  158148 main.go:141] libmachine: Using SSH client type: native
I0128 17:37:52.463161  158148 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 17:37:52.463186  158148 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0128 17:37:52.664670  158148 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0128 17:37:52.664757  158148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 17:37:52.690750  158148 main.go:141] libmachine: Using SSH client type: native
I0128 17:37:52.691585  158148 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 17:37:52.691641  158148 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0128 17:37:52.869032  158148 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0128 17:37:52.869065  158148 ubuntu.go:175] set auth options {CertDir:/home/eli/.minikube CaCertPath:/home/eli/.minikube/certs/ca.pem CaPrivateKeyPath:/home/eli/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/eli/.minikube/machines/server.pem ServerKeyPath:/home/eli/.minikube/machines/server-key.pem ClientKeyPath:/home/eli/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/eli/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/eli/.minikube}
I0128 17:37:52.869099  158148 ubuntu.go:177] setting up certificates
I0128 17:37:52.869114  158148 provision.go:83] configureAuth start
I0128 17:37:52.869242  158148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0128 17:37:52.912094  158148 provision.go:138] copyHostCerts
I0128 17:37:52.912158  158148 exec_runner.go:144] found /home/eli/.minikube/ca.pem, removing ...
I0128 17:37:52.912169  158148 exec_runner.go:203] rm: /home/eli/.minikube/ca.pem
I0128 17:37:52.912248  158148 exec_runner.go:151] cp: /home/eli/.minikube/certs/ca.pem --> /home/eli/.minikube/ca.pem (1070 bytes)
I0128 17:37:52.912384  158148 exec_runner.go:144] found /home/eli/.minikube/cert.pem, removing ...
I0128 17:37:52.912390  158148 exec_runner.go:203] rm: /home/eli/.minikube/cert.pem
I0128 17:37:52.912436  158148 exec_runner.go:151] cp: /home/eli/.minikube/certs/cert.pem --> /home/eli/.minikube/cert.pem (1115 bytes)
I0128 17:37:52.912529  158148 exec_runner.go:144] found /home/eli/.minikube/key.pem, removing ...
I0128 17:37:52.912534  158148 exec_runner.go:203] rm: /home/eli/.minikube/key.pem
I0128 17:37:52.912577  158148 exec_runner.go:151] cp: /home/eli/.minikube/certs/key.pem --> /home/eli/.minikube/key.pem (1679 bytes)
I0128 17:37:52.912655  158148 provision.go:112] generating server cert: /home/eli/.minikube/machines/server.pem ca-key=/home/eli/.minikube/certs/ca.pem private-key=/home/eli/.minikube/certs/ca-key.pem org=eli.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0128 17:37:53.493366  158148 provision.go:172] copyRemoteCerts
I0128 17:37:53.493429  158148 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0128 17:37:53.493473  158148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 17:37:53.513274  158148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/eli/.minikube/machines/minikube/id_rsa Username:docker}
I0128 17:37:53.625324  158148 ssh_runner.go:362] scp /home/eli/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0128 17:37:53.659773  158148 ssh_runner.go:362] scp /home/eli/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0128 17:37:53.698513  158148 ssh_runner.go:362] scp /home/eli/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0128 17:37:53.745032  158148 provision.go:86] duration metric: configureAuth took 875.898676ms
I0128 17:37:53.745051  158148 ubuntu.go:193] setting minikube options for container-runtime
I0128 17:37:53.745274  158148 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0128 17:37:53.745343  158148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 17:37:53.770702  158148 main.go:141] libmachine: Using SSH client type: native
I0128 17:37:53.771426  158148 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 17:37:53.771439  158148 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0128 17:37:53.946456  158148 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0128 17:37:53.946478  158148 ubuntu.go:71] root file system type: overlay
I0128 17:37:53.946700  158148 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0128 17:37:53.946831  158148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 17:37:53.989110  158148 main.go:141] libmachine: Using SSH client type: native
I0128 17:37:53.989900  158148 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 17:37:53.990254  158148 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0128 17:37:54.206706  158148 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0128 17:37:54.206850  158148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 17:37:54.245700  158148 main.go:141] libmachine: Using SSH client type: native
I0128 17:37:54.246297  158148 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0128 17:37:54.246317  158148 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0128 17:37:54.426228  158148 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0128 17:37:54.426241  158148 machine.go:91] provisioned docker machine in 1.987178708s
I0128 17:37:54.426249  158148 start.go:300] post-start starting for "minikube" (driver="docker")
I0128 17:37:54.426258  158148 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0128 17:37:54.426323  158148 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0128 17:37:54.426373  158148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 17:37:54.448224  158148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/eli/.minikube/machines/minikube/id_rsa Username:docker}
I0128 17:37:54.577492  158148 ssh_runner.go:195] Run: cat /etc/os-release
I0128 17:37:54.584365  158148 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0128 17:37:54.584421  158148 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0128 17:37:54.584449  158148 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0128 17:37:54.584458  158148 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I0128 17:37:54.584470  158148 filesync.go:126] Scanning /home/eli/.minikube/addons for local assets ...
I0128 17:37:54.584557  158148 filesync.go:126] Scanning /home/eli/.minikube/files for local assets ...
I0128 17:37:54.584601  158148 start.go:303] post-start completed in 158.344294ms
I0128 17:37:54.584671  158148 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0128 17:37:54.584730  158148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 17:37:54.606575  158148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/eli/.minikube/machines/minikube/id_rsa Username:docker}
I0128 17:37:54.720073  158148 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0128 17:37:54.735945  158148 fix.go:56] fixHost completed within 2.32308698s
I0128 17:37:54.735969  158148 start.go:83] releasing machines lock for "minikube", held for 2.323134464s
I0128 17:37:54.736176  158148 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0128 17:37:54.772846  158148 ssh_runner.go:195] Run: cat /version.json
I0128 17:37:54.772874  158148 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0128 17:37:54.772891  158148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 17:37:54.772940  158148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 17:37:54.792511  158148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/eli/.minikube/machines/minikube/id_rsa Username:docker}
I0128 17:37:54.792994  158148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/eli/.minikube/machines/minikube/id_rsa Username:docker}
I0128 17:37:54.895923  158148 ssh_runner.go:195] Run: systemctl --version
I0128 17:37:55.529627  158148 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0128 17:37:55.544664  158148 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0128 17:37:55.587662  158148 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0128 17:37:55.587736  158148 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0128 17:37:55.602183  158148 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0128 17:37:55.602196  158148 start.go:466] detecting cgroup driver to use...
I0128 17:37:55.602223  158148 detect.go:199] detected "systemd" cgroup driver on host os
I0128 17:37:55.602313  158148 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0128 17:37:55.625996  158148 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0128 17:37:55.640662  158148 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0128 17:37:55.655427  158148 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I0128 17:37:55.655493  158148 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0128 17:37:55.669951  158148 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0128 17:37:55.685289  158148 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0128 17:37:55.699677  158148 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0128 17:37:55.714776  158148 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0128 17:37:55.727952  158148 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0128 17:37:55.742190  158148 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0128 17:37:55.754413  158148 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0128 17:37:55.766504  158148 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 17:37:56.012253  158148 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0128 17:38:06.448579  158148 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (10.436254455s)
I0128 17:38:06.448603  158148 start.go:466] detecting cgroup driver to use...
I0128 17:38:06.448648  158148 detect.go:199] detected "systemd" cgroup driver on host os
I0128 17:38:06.448710  158148 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0128 17:38:06.494655  158148 cruntime.go:276] skipping containerd shutdown because we are bound to it
I0128 17:38:06.494792  158148 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0128 17:38:06.525032  158148 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0128 17:38:06.551363  158148 ssh_runner.go:195] Run: which cri-dockerd
I0128 17:38:06.556904  158148 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0128 17:38:06.569582  158148 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0128 17:38:06.601384  158148 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0128 17:38:06.755241  158148 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0128 17:38:06.909005  158148 docker.go:535] configuring docker to use "systemd" as cgroup driver...
I0128 17:38:06.909028  158148 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (143 bytes)
I0128 17:38:06.933609  158148 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 17:38:07.087044  158148 ssh_runner.go:195] Run: sudo systemctl restart docker
I0128 17:38:08.870165  158148 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.783071199s)
I0128 17:38:08.870315  158148 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0128 17:38:09.075639  158148 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0128 17:38:09.217131  158148 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0128 17:38:09.393026  158148 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 17:38:09.561129  158148 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0128 17:38:09.635242  158148 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 17:38:09.813351  158148 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0128 17:38:09.966310  158148 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0128 17:38:09.966403  158148 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0128 17:38:09.974602  158148 start.go:534] Will wait 60s for crictl version
I0128 17:38:09.974688  158148 ssh_runner.go:195] Run: which crictl
I0128 17:38:09.982844  158148 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0128 17:38:10.056850  158148 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I0128 17:38:10.056906  158148 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0128 17:38:10.084442  158148 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0128 17:38:10.114001  158148 out.go:204] üê≥  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I0128 17:38:10.114099  158148 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0128 17:38:10.134568  158148 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0128 17:38:10.139819  158148 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I0128 17:38:10.139887  158148 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0128 17:38:10.162792  158148 docker.go:636] Got preloaded images: -- stdout --
apache/apisix:3.6.0-debian
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
bitnami/etcd:3.5.7-debian-11-r14
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
apache/apisix-ingress-controller:1.6.0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0128 17:38:10.162807  158148 docker.go:566] Images already preloaded, skipping extraction
I0128 17:38:10.162863  158148 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0128 17:38:10.185953  158148 docker.go:636] Got preloaded images: -- stdout --
apache/apisix:3.6.0-debian
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
bitnami/etcd:3.5.7-debian-11-r14
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
apache/apisix-ingress-controller:1.6.0
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0128 17:38:10.185969  158148 cache_images.go:84] Images are preloaded, skipping loading
I0128 17:38:10.186064  158148 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0128 17:38:10.249498  158148 cni.go:84] Creating CNI manager for ""
I0128 17:38:10.249519  158148 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0128 17:38:10.249528  158148 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0128 17:38:10.249546  158148 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0128 17:38:10.249671  158148 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0128 17:38:10.249732  158148 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0128 17:38:10.249788  158148 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I0128 17:38:10.262240  158148 binaries.go:44] Found k8s binaries, skipping transfer
I0128 17:38:10.262298  158148 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0128 17:38:10.274774  158148 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0128 17:38:10.299215  158148 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0128 17:38:10.324343  158148 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I0128 17:38:10.350795  158148 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0128 17:38:10.355505  158148 certs.go:56] Setting up /home/eli/.minikube/profiles/minikube for IP: 192.168.49.2
I0128 17:38:10.355521  158148 certs.go:190] acquiring lock for shared ca certs: {Name:mkf701f63847de7d58d8d96dfd90da29ad445512 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 17:38:10.355679  158148 certs.go:199] skipping minikubeCA CA generation: /home/eli/.minikube/ca.key
I0128 17:38:10.355749  158148 certs.go:199] skipping proxyClientCA CA generation: /home/eli/.minikube/proxy-client-ca.key
I0128 17:38:10.355858  158148 certs.go:315] skipping minikube-user signed cert generation: /home/eli/.minikube/profiles/minikube/client.key
I0128 17:38:10.355937  158148 certs.go:315] skipping minikube signed cert generation: /home/eli/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0128 17:38:10.356006  158148 certs.go:315] skipping aggregator signed cert generation: /home/eli/.minikube/profiles/minikube/proxy-client.key
I0128 17:38:10.356214  158148 certs.go:437] found cert: /home/eli/.minikube/certs/home/eli/.minikube/certs/ca-key.pem (1675 bytes)
I0128 17:38:10.356262  158148 certs.go:437] found cert: /home/eli/.minikube/certs/home/eli/.minikube/certs/ca.pem (1070 bytes)
I0128 17:38:10.356303  158148 certs.go:437] found cert: /home/eli/.minikube/certs/home/eli/.minikube/certs/cert.pem (1115 bytes)
I0128 17:38:10.356345  158148 certs.go:437] found cert: /home/eli/.minikube/certs/home/eli/.minikube/certs/key.pem (1679 bytes)
I0128 17:38:10.357140  158148 ssh_runner.go:362] scp /home/eli/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0128 17:38:10.390670  158148 ssh_runner.go:362] scp /home/eli/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0128 17:38:10.425764  158148 ssh_runner.go:362] scp /home/eli/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0128 17:38:10.464789  158148 ssh_runner.go:362] scp /home/eli/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0128 17:38:10.509154  158148 ssh_runner.go:362] scp /home/eli/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0128 17:38:10.557666  158148 ssh_runner.go:362] scp /home/eli/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0128 17:38:10.603741  158148 ssh_runner.go:362] scp /home/eli/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0128 17:38:10.653451  158148 ssh_runner.go:362] scp /home/eli/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0128 17:38:10.699626  158148 ssh_runner.go:362] scp /home/eli/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0128 17:38:10.747053  158148 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0128 17:38:10.781152  158148 ssh_runner.go:195] Run: openssl version
I0128 17:38:10.790932  158148 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0128 17:38:10.811194  158148 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0128 17:38:10.818997  158148 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov  2 21:27 /usr/share/ca-certificates/minikubeCA.pem
I0128 17:38:10.819068  158148 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0128 17:38:10.834968  158148 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0128 17:38:10.855456  158148 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0128 17:38:10.860368  158148 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0128 17:38:10.869241  158148 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0128 17:38:10.877967  158148 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0128 17:38:10.886561  158148 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0128 17:38:10.895118  158148 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0128 17:38:10.903736  158148 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0128 17:38:10.912431  158148 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:7900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/eli:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I0128 17:38:10.912543  158148 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0128 17:38:10.935321  158148 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0128 17:38:10.948338  158148 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0128 17:38:10.948349  158148 kubeadm.go:636] restartCluster start
I0128 17:38:10.948401  158148 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0128 17:38:10.960386  158148 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0128 17:38:10.961122  158148 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I0128 17:38:10.963209  158148 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0128 17:38:10.975359  158148 api_server.go:166] Checking apiserver status ...
I0128 17:38:10.975415  158148 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 17:38:10.989690  158148 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 17:38:10.989699  158148 api_server.go:166] Checking apiserver status ...
I0128 17:38:10.989748  158148 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 17:38:11.004001  158148 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 17:38:11.504644  158148 api_server.go:166] Checking apiserver status ...
I0128 17:38:11.504750  158148 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0128 17:38:11.530091  158148 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0128 17:38:12.004895  158148 api_server.go:166] Checking apiserver status ...
I0128 17:38:12.004986  158148 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 17:38:12.031856  158148 ssh_runner.go:195] Run: sudo egrep ^[0-9]+:freezer: /proc/79534/cgroup
W0128 17:38:12.054890  158148 api_server.go:177] unable to find freezer cgroup: sudo egrep ^[0-9]+:freezer: /proc/79534/cgroup: Process exited with status 1
stdout:

stderr:
I0128 17:38:12.054978  158148 ssh_runner.go:195] Run: ls
I0128 17:38:12.062263  158148 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 17:38:14.847814  158148 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0128 17:38:14.847845  158148 retry.go:31] will retry after 299.401545ms: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0128 17:38:15.148120  158148 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 17:38:15.155226  158148 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 17:38:15.155249  158148 retry.go:31] will retry after 310.497309ms: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 17:38:15.466838  158148 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 17:38:15.481099  158148 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 17:38:15.481144  158148 retry.go:31] will retry after 466.823926ms: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 17:38:15.948434  158148 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 17:38:15.958166  158148 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 17:38:15.958201  158148 retry.go:31] will retry after 554.183466ms: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0128 17:38:16.513358  158148 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 17:38:16.526255  158148 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0128 17:38:16.571971  158148 system_pods.go:86] 7 kube-system pods found
I0128 17:38:16.572012  158148 system_pods.go:89] "coredns-5d78c9869d-49fn2" [70880a1f-3d79-4869-9172-25ecab22167d] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0128 17:38:16.572072  158148 system_pods.go:89] "etcd-minikube" [5b9f92c6-75de-4e7e-a332-ad67af11152b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0128 17:38:16.572110  158148 system_pods.go:89] "kube-apiserver-minikube" [4aebfe65-10a1-4734-95d9-9dcae1dad930] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0128 17:38:16.572130  158148 system_pods.go:89] "kube-controller-manager-minikube" [a2930386-8a8d-4fec-810d-d0e3fac7a955] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0128 17:38:16.572142  158148 system_pods.go:89] "kube-proxy-vtfrm" [c0fe3336-d90a-4241-b964-4d3014ef7b4a] Running
I0128 17:38:16.572161  158148 system_pods.go:89] "kube-scheduler-minikube" [211c0ac2-b7dd-4fa3-8e63-ffac109bf1c3] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0128 17:38:16.572177  158148 system_pods.go:89] "storage-provisioner" [dafca340-9fc1-4fcf-8962-c552e72b3d80] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0128 17:38:16.574894  158148 api_server.go:141] control plane version: v1.27.4
I0128 17:38:16.574920  158148 kubeadm.go:630] The running cluster does not require reconfiguration: 192.168.49.2
I0128 17:38:16.574936  158148 kubeadm.go:684] Taking a shortcut, as the cluster seems to be properly configured
I0128 17:38:16.574945  158148 kubeadm.go:640] restartCluster took 5.626588916s
I0128 17:38:16.574954  158148 kubeadm.go:406] StartCluster complete in 5.662528477s
I0128 17:38:16.574977  158148 settings.go:142] acquiring lock: {Name:mk2b89b6030a0052e855e30f1b630d141ba1ac07 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 17:38:16.575095  158148 settings.go:150] Updating kubeconfig:  /home/eli/.kube/config
I0128 17:38:16.577602  158148 lock.go:35] WriteFile acquiring /home/eli/.kube/config: {Name:mk9e9ce32ea3f6bac7b45fa23a79c1a1593c6c8a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 17:38:16.578126  158148 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0128 17:38:16.578294  158148 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I0128 17:38:16.578528  158148 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0128 17:38:16.578573  158148 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0128 17:38:16.578590  158148 addons.go:240] addon storage-provisioner should already be in state true
I0128 17:38:16.578606  158148 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0128 17:38:16.578621  158148 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I0128 17:38:16.578641  158148 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0128 17:38:16.578691  158148 host.go:66] Checking if "minikube" exists ...
I0128 17:38:16.579315  158148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 17:38:16.579713  158148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 17:38:16.586469  158148 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0128 17:38:16.586532  158148 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I0128 17:38:16.589501  158148 out.go:177] üîé  Verifying Kubernetes components...
I0128 17:38:16.590650  158148 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0128 17:38:16.621475  158148 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0128 17:38:16.622995  158148 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0128 17:38:16.623011  158148 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0128 17:38:16.623096  158148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 17:38:16.627993  158148 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0128 17:38:16.628007  158148 addons.go:240] addon default-storageclass should already be in state true
I0128 17:38:16.628048  158148 host.go:66] Checking if "minikube" exists ...
I0128 17:38:16.628967  158148 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 17:38:16.648732  158148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/eli/.minikube/machines/minikube/id_rsa Username:docker}
I0128 17:38:16.653344  158148 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0128 17:38:16.653354  158148 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0128 17:38:16.653412  158148 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 17:38:16.676260  158148 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/eli/.minikube/machines/minikube/id_rsa Username:docker}
I0128 17:38:16.704727  158148 api_server.go:52] waiting for apiserver process to appear ...
I0128 17:38:16.704758  158148 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0128 17:38:16.704786  158148 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 17:38:16.722821  158148 api_server.go:72] duration metric: took 136.239393ms to wait for apiserver process to appear ...
I0128 17:38:16.722833  158148 api_server.go:88] waiting for apiserver healthz status ...
I0128 17:38:16.722845  158148 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0128 17:38:16.728424  158148 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0128 17:38:16.729417  158148 api_server.go:141] control plane version: v1.27.4
I0128 17:38:16.729428  158148 api_server.go:131] duration metric: took 6.590249ms to wait for apiserver health ...
I0128 17:38:16.729434  158148 system_pods.go:43] waiting for kube-system pods to appear ...
I0128 17:38:16.738721  158148 system_pods.go:59] 7 kube-system pods found
I0128 17:38:16.738739  158148 system_pods.go:61] "coredns-5d78c9869d-49fn2" [70880a1f-3d79-4869-9172-25ecab22167d] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0128 17:38:16.738751  158148 system_pods.go:61] "etcd-minikube" [5b9f92c6-75de-4e7e-a332-ad67af11152b] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0128 17:38:16.738761  158148 system_pods.go:61] "kube-apiserver-minikube" [4aebfe65-10a1-4734-95d9-9dcae1dad930] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0128 17:38:16.738772  158148 system_pods.go:61] "kube-controller-manager-minikube" [a2930386-8a8d-4fec-810d-d0e3fac7a955] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0128 17:38:16.738779  158148 system_pods.go:61] "kube-proxy-vtfrm" [c0fe3336-d90a-4241-b964-4d3014ef7b4a] Running
I0128 17:38:16.738788  158148 system_pods.go:61] "kube-scheduler-minikube" [211c0ac2-b7dd-4fa3-8e63-ffac109bf1c3] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0128 17:38:16.738806  158148 system_pods.go:61] "storage-provisioner" [dafca340-9fc1-4fcf-8962-c552e72b3d80] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0128 17:38:16.738812  158148 system_pods.go:74] duration metric: took 9.37287ms to wait for pod list to return data ...
I0128 17:38:16.738822  158148 kubeadm.go:581] duration metric: took 152.243354ms to wait for : map[apiserver:true system_pods:true] ...
I0128 17:38:16.738834  158148 node_conditions.go:102] verifying NodePressure condition ...
I0128 17:38:16.742122  158148 node_conditions.go:122] node storage ephemeral capacity is 958802032Ki
I0128 17:38:16.742136  158148 node_conditions.go:123] node cpu capacity is 12
I0128 17:38:16.742146  158148 node_conditions.go:105] duration metric: took 3.308499ms to run NodePressure ...
I0128 17:38:16.742157  158148 start.go:228] waiting for startup goroutines ...
I0128 17:38:16.772972  158148 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0128 17:38:16.804670  158148 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0128 17:38:17.683404  158148 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0128 17:38:17.685651  158148 addons.go:502] enable addons completed in 1.107392364s: enabled=[storage-provisioner default-storageclass]
I0128 17:38:17.685677  158148 start.go:233] waiting for cluster config update ...
I0128 17:38:17.685691  158148 start.go:242] writing updated cluster config ...
I0128 17:38:17.686024  158148 ssh_runner.go:195] Run: rm -f paused
I0128 17:38:17.759619  158148 start.go:600] kubectl: 1.28.5, cluster: 1.27.4 (minor skew: 1)
I0128 17:38:17.761073  158148 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Jan 28 20:38:09 minikube systemd[1]: cri-docker.service: Consumed 1min 44.670s CPU time.
Jan 28 20:38:09 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jan 28 20:38:09 minikube cri-dockerd[78214]: time="2024-01-28T20:38:09Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jan 28 20:38:09 minikube cri-dockerd[78214]: time="2024-01-28T20:38:09Z" level=info msg="Start docker client with request timeout 0s"
Jan 28 20:38:09 minikube cri-dockerd[78214]: time="2024-01-28T20:38:09Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jan 28 20:38:09 minikube cri-dockerd[78214]: time="2024-01-28T20:38:09Z" level=info msg="Loaded network plugin cni"
Jan 28 20:38:09 minikube cri-dockerd[78214]: time="2024-01-28T20:38:09Z" level=info msg="Docker cri networking managed by network plugin cni"
Jan 28 20:38:09 minikube cri-dockerd[78214]: time="2024-01-28T20:38:09Z" level=info msg="Docker Info: &{ID:624d99ac-ebcd-49f9-adf0-6b57c9bc674e Containers:24 ContainersRunning:0 ContainersPaused:0 ContainersStopped:24 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:false NGoroutines:35 SystemTime:2024-01-28T20:38:09.718667749Z LoggingDriver:json-file CgroupDriver:systemd CgroupVersion:2 NEventsListener:0 KernelVersion:6.1.0-16-amd64 OperatingSystem:Ubuntu 22.04.2 LTS OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc000151650 NCPU:12 MemTotal:33458995200 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: DefaultAddressPools:[] Warnings:[]}"
Jan 28 20:38:09 minikube cri-dockerd[78214]: time="2024-01-28T20:38:09Z" level=info msg="Setting cgroupDriver systemd"
Jan 28 20:38:09 minikube cri-dockerd[78214]: time="2024-01-28T20:38:09Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jan 28 20:38:09 minikube cri-dockerd[78214]: time="2024-01-28T20:38:09Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jan 28 20:38:09 minikube cri-dockerd[78214]: time="2024-01-28T20:38:09Z" level=info msg="Start cri-dockerd grpc backend"
Jan 28 20:38:09 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jan 28 20:38:09 minikube systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
Jan 28 20:38:09 minikube systemd[1]: cri-docker.service: Deactivated successfully.
Jan 28 20:38:09 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
Jan 28 20:38:09 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jan 28 20:38:09 minikube cri-dockerd[78304]: time="2024-01-28T20:38:09Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jan 28 20:38:09 minikube cri-dockerd[78304]: time="2024-01-28T20:38:09Z" level=info msg="Start docker client with request timeout 0s"
Jan 28 20:38:09 minikube cri-dockerd[78304]: time="2024-01-28T20:38:09Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jan 28 20:38:09 minikube cri-dockerd[78304]: time="2024-01-28T20:38:09Z" level=info msg="Loaded network plugin cni"
Jan 28 20:38:09 minikube cri-dockerd[78304]: time="2024-01-28T20:38:09Z" level=info msg="Docker cri networking managed by network plugin cni"
Jan 28 20:38:09 minikube cri-dockerd[78304]: time="2024-01-28T20:38:09Z" level=info msg="Docker Info: &{ID:624d99ac-ebcd-49f9-adf0-6b57c9bc674e Containers:24 ContainersRunning:0 ContainersPaused:0 ContainersStopped:24 Images:11 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:false NGoroutines:36 SystemTime:2024-01-28T20:38:09.948300398Z LoggingDriver:json-file CgroupDriver:systemd CgroupVersion:2 NEventsListener:0 KernelVersion:6.1.0-16-amd64 OperatingSystem:Ubuntu 22.04.2 LTS OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0004e8150 NCPU:12 MemTotal:33458995200 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.4 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:3dce8eb055cbb6872793272b4f20ed16117344f8 Expected:3dce8eb055cbb6872793272b4f20ed16117344f8} RuncCommit:{ID:v1.1.7-0-g860f061 Expected:v1.1.7-0-g860f061} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: DefaultAddressPools:[] Warnings:[]}"
Jan 28 20:38:09 minikube cri-dockerd[78304]: time="2024-01-28T20:38:09Z" level=info msg="Setting cgroupDriver systemd"
Jan 28 20:38:09 minikube cri-dockerd[78304]: time="2024-01-28T20:38:09Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jan 28 20:38:09 minikube cri-dockerd[78304]: time="2024-01-28T20:38:09Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jan 28 20:38:09 minikube cri-dockerd[78304]: time="2024-01-28T20:38:09Z" level=info msg="Start cri-dockerd grpc backend"
Jan 28 20:38:09 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jan 28 20:38:11 minikube cri-dockerd[78304]: time="2024-01-28T20:38:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3c03d576a27c5c498d5be6f752668611d2385d58c6700f27d98cb6c963f7ef87/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Jan 28 20:38:11 minikube cri-dockerd[78304]: time="2024-01-28T20:38:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/be395ba8f65df7ea57fa4de52eee60fd115ecd5101f2de3ee6786386fcc0c8af/resolv.conf as [nameserver 10.96.0.10 search ingress-apisix.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 28 20:38:11 minikube cri-dockerd[78304]: time="2024-01-28T20:38:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1c1c2e898982f562b837dab8dd1d6a9e939f5ff0425214a5f05f39439f10afe4/resolv.conf as [nameserver 10.96.0.10 search ingress-apisix.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 28 20:38:11 minikube cri-dockerd[78304]: time="2024-01-28T20:38:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/952aa5d0a01e75b09991aeec99684c8e55efd7502fa154dae50034f18ebfd9bf/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Jan 28 20:38:11 minikube cri-dockerd[78304]: time="2024-01-28T20:38:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2f10e65ef7a5c01198e6727ea2560e2ee5c1e31ac16d5824aae06c8e6abe76c3/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Jan 28 20:38:11 minikube cri-dockerd[78304]: time="2024-01-28T20:38:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c40e10aeff73dc3564acd36b62df94a7745d24fbd7e4c68997bdbf3f5e4fe651/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Jan 28 20:38:11 minikube cri-dockerd[78304]: time="2024-01-28T20:38:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2cb1a60662b27b0096721311919e7442ee218f110ababbe5a2c00d31d8517e94/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Jan 28 20:38:11 minikube cri-dockerd[78304]: time="2024-01-28T20:38:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ffe4d988dd4a415ac2b085403dd80ac5fae21ec4237365ab8f9de8b8b1e1e24b/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Jan 28 20:38:11 minikube cri-dockerd[78304]: time="2024-01-28T20:38:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2bd98cf6129ab9019914732d9b09f56906f330c43dd601ab5244a3590cde8f49/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Jan 28 20:38:11 minikube cri-dockerd[78304]: time="2024-01-28T20:38:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4634d8fd63a52141887e5c19bb4c38b245b98f7bdf2361f2eebe16eb08720f13/resolv.conf as [nameserver 10.96.0.10 search ingress-apisix.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 28 20:38:11 minikube dockerd[77993]: time="2024-01-28T20:38:11.873347503Z" level=info msg="ignoring event" container=c71dd39b877b6ff5cab591786638f4771c3d90e1cb02ac287204cbd14579a792 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 28 20:38:11 minikube cri-dockerd[78304]: time="2024-01-28T20:38:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9777ed8b0a699a573eb900373a4dd657939801e0b9a730987787b04a0afd6ba5/resolv.conf as [nameserver 10.96.0.10 search ingress-apisix.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 28 20:38:11 minikube cri-dockerd[78304]: time="2024-01-28T20:38:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/36f86bccaa818d9531ec15eb2825dd333a222ddb3d8b319281608049e8f1a989/resolv.conf as [nameserver 10.96.0.10 search ingress-apisix.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 28 20:38:17 minikube cri-dockerd[78304]: time="2024-01-28T20:38:17Z" level=info msg="Stop pulling image busybox:1.28: Status: Downloaded newer image for busybox:1.28"
Jan 28 20:38:20 minikube cri-dockerd[78304]: time="2024-01-28T20:38:20Z" level=info msg="Stop pulling image busybox:1.28: Status: Image is up to date for busybox:1.28"
Jan 28 20:39:16 minikube dockerd[77993]: time="2024-01-28T20:39:16.179670495Z" level=info msg="ignoring event" container=918b53d7e4e6e2b8b35bc00197c2446e19b92485fdb7365a095ba08985345765 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 28 20:39:36 minikube dockerd[77993]: time="2024-01-28T20:39:36.561258630Z" level=info msg="ignoring event" container=8ef32054757b11c19d7de8bb1011524fafb22f1af3caa000fe1ac6e72839b6a3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 28 20:40:17 minikube dockerd[77993]: time="2024-01-28T20:40:17.372150071Z" level=warning msg="no trace recorder found, skipping"
Jan 28 20:41:09 minikube cri-dockerd[78304]: time="2024-01-28T20:41:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/549cf2d0bf848a699e9b88190e34863dd0664028d0e915f6de54b8e5db24386c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 28 20:41:09 minikube cri-dockerd[78304]: time="2024-01-28T20:41:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1eb47b0ca3461fe749db9bed61b82f675613f00766ea8b025740f20eddae7bf3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 28 20:41:12 minikube dockerd[77993]: time="2024-01-28T20:41:12.333943188Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 28 20:41:12 minikube dockerd[77993]: time="2024-01-28T20:41:12.334023469Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 28 20:41:15 minikube dockerd[77993]: time="2024-01-28T20:41:15.408140818Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 28 20:41:15 minikube dockerd[77993]: time="2024-01-28T20:41:15.408224949Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 28 20:41:27 minikube dockerd[77993]: time="2024-01-28T20:41:27.393826916Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 28 20:41:27 minikube dockerd[77993]: time="2024-01-28T20:41:27.393920174Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 28 20:41:33 minikube dockerd[77993]: time="2024-01-28T20:41:33.642152455Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 28 20:41:33 minikube dockerd[77993]: time="2024-01-28T20:41:33.642225117Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 28 20:41:58 minikube dockerd[77993]: time="2024-01-28T20:41:58.519041018Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 28 20:41:58 minikube dockerd[77993]: time="2024-01-28T20:41:58.519076436Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 28 20:42:01 minikube dockerd[77993]: time="2024-01-28T20:42:01.180802098Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 28 20:42:01 minikube dockerd[77993]: time="2024-01-28T20:42:01.180887188Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                      CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
b85887f24bd74       b413897e9d3e3                                                                                              2 minutes ago       Running             ingress-controller        2                   36f86bccaa818       apisix-ingress-controller-78ff6d8549-jk7v8
bdd6828f12b5c       6d2ba652c0109                                                                                              3 minutes ago       Running             apisix                    2                   1c1c2e898982f       apisix-bf7895f94-q4mlp
b8b40f8ea88db       6e38f40d628db                                                                                              4 minutes ago       Running             storage-provisioner       8                   2f10e65ef7a5c       storage-provisioner
8ef32054757b1       busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47                            4 minutes ago       Exited              wait-apisix-admin         3                   36f86bccaa818       apisix-ingress-controller-78ff6d8549-jk7v8
918b53d7e4e6e       busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47                            4 minutes ago       Exited              wait-etcd                 3                   1c1c2e898982f       apisix-bf7895f94-q4mlp
a5a793beef7d0       6848d7eda0341                                                                                              4 minutes ago       Running             kube-proxy                4                   2bd98cf6129ab       kube-proxy-vtfrm
07031615dc2fd       73feeccc2b47c                                                                                              4 minutes ago       Running             etcd                      3                   4634d8fd63a52       apisix-etcd-1
b00b6187d796a       73feeccc2b47c                                                                                              4 minutes ago       Running             etcd                      3                   9777ed8b0a699       apisix-etcd-2
b2bb4a55c13d4       f466468864b7a                                                                                              4 minutes ago       Running             kube-controller-manager   4                   ffe4d988dd4a4       kube-controller-manager-minikube
eccecc878eeee       ead0a4a53df89                                                                                              4 minutes ago       Running             coredns                   5                   2cb1a60662b27       coredns-5d78c9869d-49fn2
a0515ef698845       e7972205b6614                                                                                              4 minutes ago       Running             kube-apiserver            4                   c40e10aeff73d       kube-apiserver-minikube
0a2352ad1b6fd       73feeccc2b47c                                                                                              4 minutes ago       Running             etcd                      3                   be395ba8f65df       apisix-etcd-0
c71dd39b877b6       6e38f40d628db                                                                                              4 minutes ago       Exited              storage-provisioner       7                   2f10e65ef7a5c       storage-provisioner
4df0cd77598a9       98ef2570f3cde                                                                                              4 minutes ago       Running             kube-scheduler            4                   3c03d576a27c5       kube-scheduler-minikube
989b491f89824       86b6af7dd652c                                                                                              4 minutes ago       Running             etcd                      4                   952aa5d0a01e7       etcd-minikube
1690eae8cf9ad       apache/apisix-ingress-controller@sha256:b65e13237dbb2ab19e7eccc7cc1afadc679096e0906108b01a907f12895465e3   37 minutes ago      Exited              ingress-controller        1                   88817ecdc7614       apisix-ingress-controller-78ff6d8549-jk7v8
88365c5424129       apache/apisix@sha256:f61f941685bd4ddecccb64bf2e5bf4954da021ff5e1210a5bcd8c8c9c5757295                      37 minutes ago      Exited              apisix                    1                   86aac8934e3c0       apisix-bf7895f94-q4mlp
dc4465e1b4552       73feeccc2b47c                                                                                              38 minutes ago      Exited              etcd                      2                   aa42ddf3441f4       apisix-etcd-1
337414b98f6ca       73feeccc2b47c                                                                                              38 minutes ago      Exited              etcd                      2                   a301c0f4757c6       apisix-etcd-2
7fc52855b8309       73feeccc2b47c                                                                                              38 minutes ago      Exited              etcd                      2                   56dab6730a988       apisix-etcd-0
c43ebcb622e9e       ead0a4a53df89                                                                                              38 minutes ago      Exited              coredns                   4                   3a461cfbfaa93       coredns-5d78c9869d-49fn2
8d9bf3b02acf3       6848d7eda0341                                                                                              39 minutes ago      Exited              kube-proxy                3                   81c4ffdde39c9       kube-proxy-vtfrm
7a8017edeef25       e7972205b6614                                                                                              39 minutes ago      Exited              kube-apiserver            3                   dbdcf40506bad       kube-apiserver-minikube
9486f545da691       86b6af7dd652c                                                                                              39 minutes ago      Exited              etcd                      3                   cbdd1acaf608f       etcd-minikube
edd74f96dec5f       f466468864b7a                                                                                              39 minutes ago      Exited              kube-controller-manager   3                   9dfaafee7ed0d       kube-controller-manager-minikube
b090209ba3bc0       98ef2570f3cde                                                                                              39 minutes ago      Exited              kube-scheduler            3                   118eef89b4c08       kube-scheduler-minikube

* 
* ==> coredns [c43ebcb622e9] <==
* [INFO] 10.244.0.20:38737 - 40738 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000215453s
[INFO] 10.244.0.20:56152 - 36930 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000360831s
[INFO] 10.244.0.22:42406 - 54463 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000279568s
[INFO] 10.244.0.22:54082 - 4844 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000531472s
[INFO] 10.244.0.22:53054 - 58308 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000216125s
[INFO] 10.244.0.22:57769 - 63947 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000436944s
[INFO] 10.244.0.23:37407 - 11448 "A IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000357772s
[INFO] 10.244.0.23:42523 - 62308 "AAAA IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000489275s
[INFO] 10.244.0.23:46361 - 6282 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000190258s
[INFO] 10.244.0.23:40052 - 36178 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000312303s
[INFO] 10.244.0.24:35299 - 33160 "A IN apisix-admin.ingress-apisix.svc.cluster.local.ingress-apisix.svc.cluster.local. udp 96 false 512" NXDOMAIN qr,aa,rd 189 0.000111867s
[INFO] 10.244.0.24:35299 - 33568 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.ingress-apisix.svc.cluster.local. udp 96 false 512" NXDOMAIN qr,aa,rd 189 0.000185134s
[INFO] 10.244.0.24:53993 - 37294 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000083137s
[INFO] 10.244.0.24:53993 - 36714 "A IN apisix-admin.ingress-apisix.svc.cluster.local.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000196933s
[INFO] 10.244.0.24:53604 - 12962 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000086221s
[INFO] 10.244.0.24:53604 - 12644 "A IN apisix-admin.ingress-apisix.svc.cluster.local.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000126442s
[INFO] 10.244.0.24:48682 - 31593 "A IN apisix-admin.ingress-apisix.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.00007767s
[INFO] 10.244.0.24:48682 - 31903 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000127194s
[INFO] 10.244.0.20:45811 - 31812 "A IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000192341s
[INFO] 10.244.0.20:35574 - 4437 "AAAA IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000317393s
[INFO] 10.244.0.20:33509 - 28741 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000229857s
[INFO] 10.244.0.20:56143 - 10536 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000223489s
[INFO] 10.244.0.22:44655 - 16670 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000298692s
[INFO] 10.244.0.22:38724 - 51855 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000249213s
[INFO] 10.244.0.22:44129 - 7129 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000440324s
[INFO] 10.244.0.22:36401 - 16394 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000779411s
[INFO] 10.244.0.23:50804 - 6679 "A IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000275382s
[INFO] 10.244.0.23:37421 - 42889 "AAAA IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000246074s
[INFO] 10.244.0.23:41100 - 7481 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000447955s
[INFO] 10.244.0.23:58646 - 29524 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000584708s
[INFO] 10.244.0.20:46299 - 37706 "AAAA IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000116005s
[INFO] 10.244.0.20:45836 - 5177 "A IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000190711s
[INFO] 10.244.0.20:53128 - 40952 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000119702s
[INFO] 10.244.0.20:34843 - 36614 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000208231s
[INFO] 10.244.0.22:53541 - 2522 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000159357s
[INFO] 10.244.0.22:37592 - 55474 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000199061s
[INFO] 10.244.0.22:35322 - 56099 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000107997s
[INFO] 10.244.0.22:34491 - 43779 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000173182s
[INFO] 10.244.0.24:60892 - 24748 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.ingress-apisix.svc.cluster.local. udp 96 false 512" NXDOMAIN qr,aa,rd 189 0.000259466s
[INFO] 10.244.0.24:60892 - 23710 "A IN apisix-admin.ingress-apisix.svc.cluster.local.ingress-apisix.svc.cluster.local. udp 96 false 512" NXDOMAIN qr,aa,rd 189 0.000400736s
[INFO] 10.244.0.24:37136 - 37548 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000216349s
[INFO] 10.244.0.24:37136 - 35979 "A IN apisix-admin.ingress-apisix.svc.cluster.local.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000332977s
[INFO] 10.244.0.24:39672 - 38811 "A IN apisix-admin.ingress-apisix.svc.cluster.local.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000176972s
[INFO] 10.244.0.24:39672 - 39756 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000333476s
[INFO] 10.244.0.24:60868 - 3050 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000400525s
[INFO] 10.244.0.24:60868 - 2086 "A IN apisix-admin.ingress-apisix.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000537196s
[INFO] 10.244.0.23:37197 - 27994 "A IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000097995s
[INFO] 10.244.0.23:42022 - 61379 "AAAA IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000111112s
[INFO] 10.244.0.23:51667 - 63814 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000073387s
[INFO] 10.244.0.23:56441 - 5056 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000124957s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s
[INFO] 10.244.0.22:34813 - 22625 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000140604s
[INFO] 10.244.0.22:55114 - 40933 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000224501s
[INFO] 10.244.0.22:56889 - 47604 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000106766s
[INFO] 10.244.0.22:50225 - 55861 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000073862s
[INFO] 10.244.0.23:55278 - 8772 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000102482s
[INFO] 10.244.0.23:36133 - 14128 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000080596s
[INFO] 10.244.0.22:40631 - 39272 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000127018s
[INFO] 10.244.0.22:53134 - 16583 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000211246s

* 
* ==> coredns [eccecc878eee] <==
* [INFO] 10.244.0.32:44609 - 23169 "AAAA IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000301035s
[INFO] 10.244.0.32:38787 - 44565 "A IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000490152s
[INFO] 10.244.0.32:48568 - 22834 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000228405s
[INFO] 10.244.0.32:43792 - 34470 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000390141s
[INFO] 10.244.0.29:44436 - 51825 "A IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000235108s
[INFO] 10.244.0.29:35642 - 573 "AAAA IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000373134s
[INFO] 10.244.0.33:39539 - 56305 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000256509s
[INFO] 10.244.0.33:36679 - 15004 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000697934s
[INFO] 10.244.0.29:52304 - 22052 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000224537s
[INFO] 10.244.0.29:59555 - 11116 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000352585s
[INFO] 10.244.0.33:34253 - 12239 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000159945s
[INFO] 10.244.0.33:41512 - 52227 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000277993s
[INFO] 10.244.0.34:37186 - 61824 "A IN apisix-admin.ingress-apisix.svc.cluster.local.ingress-apisix.svc.cluster.local. udp 96 false 512" NXDOMAIN qr,aa,rd 189 0.000411348s
[INFO] 10.244.0.34:37186 - 62949 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.ingress-apisix.svc.cluster.local. udp 96 false 512" NXDOMAIN qr,aa,rd 189 0.000608327s
[INFO] 10.244.0.34:42085 - 49402 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000292806s
[INFO] 10.244.0.34:42085 - 48380 "A IN apisix-admin.ingress-apisix.svc.cluster.local.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000416364s
[INFO] 10.244.0.34:48193 - 51932 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000295385s
[INFO] 10.244.0.34:48193 - 51224 "A IN apisix-admin.ingress-apisix.svc.cluster.local.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000412068s
[INFO] 10.244.0.34:55509 - 26557 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000157234s
[INFO] 10.244.0.34:55509 - 25273 "A IN apisix-admin.ingress-apisix.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000254947s
[INFO] 10.244.0.32:53132 - 42818 "AAAA IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.00019972s
[INFO] 10.244.0.32:47877 - 31375 "A IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000392348s
[INFO] 10.244.0.32:38117 - 51134 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000350392s
[INFO] 10.244.0.32:41470 - 62519 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000505178s
[INFO] 10.244.0.29:41253 - 54827 "AAAA IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000395789s
[INFO] 10.244.0.29:53312 - 20925 "A IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000579232s
[INFO] 10.244.0.29:33188 - 62204 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000225518s
[INFO] 10.244.0.33:44145 - 19685 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000206395s
[INFO] 10.244.0.33:45742 - 57823 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000358289s
[INFO] 10.244.0.29:59342 - 47192 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000623931s
[INFO] 10.244.0.33:38249 - 27176 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000169714s
[INFO] 10.244.0.33:53815 - 30787 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.00027165s
[INFO] 10.244.0.34:39536 - 31807 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.ingress-apisix.svc.cluster.local. udp 96 false 512" NXDOMAIN qr,aa,rd 189 0.000211182s
[INFO] 10.244.0.34:39536 - 30119 "A IN apisix-admin.ingress-apisix.svc.cluster.local.ingress-apisix.svc.cluster.local. udp 96 false 512" NXDOMAIN qr,aa,rd 189 0.000275607s
[INFO] 10.244.0.34:49942 - 6468 "A IN apisix-admin.ingress-apisix.svc.cluster.local.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000156613s
[INFO] 10.244.0.34:49942 - 7887 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.0003279s
[INFO] 10.244.0.34:45703 - 18495 "A IN apisix-admin.ingress-apisix.svc.cluster.local.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.00017368s
[INFO] 10.244.0.34:45703 - 19823 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000278977s
[INFO] 10.244.0.34:60217 - 27658 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000179736s
[INFO] 10.244.0.34:60217 - 26759 "A IN apisix-admin.ingress-apisix.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000214324s
[INFO] 10.244.0.32:36611 - 58731 "AAAA IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000229044s
[INFO] 10.244.0.32:50203 - 35665 "A IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000398497s
[INFO] 10.244.0.32:58042 - 4181 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000204629s
[INFO] 10.244.0.29:48352 - 34184 "A IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000244112s
[INFO] 10.244.0.32:35532 - 29401 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000248392s
[INFO] 10.244.0.29:49382 - 40951 "AAAA IN apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000363718s
[INFO] 10.244.0.33:44826 - 51397 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000288356s
[INFO] 10.244.0.33:57170 - 39537 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000450479s
[INFO] 10.244.0.29:56655 - 18122 "A IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.00026155s
[INFO] 10.244.0.29:50042 - 16380 "AAAA IN apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000412618s
[INFO] 10.244.0.33:48745 - 57145 "A IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 168 0.000227628s
[INFO] 10.244.0.33:38585 - 34219 "AAAA IN apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local. udp 85 false 512" NOERROR qr,aa,rd 178 0.000414505s
[INFO] 10.244.0.34:58769 - 20423 "A IN apisix-admin.ingress-apisix.svc.cluster.local.ingress-apisix.svc.cluster.local. udp 96 false 512" NXDOMAIN qr,aa,rd 189 0.000219425s
[INFO] 10.244.0.34:58769 - 21375 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.ingress-apisix.svc.cluster.local. udp 96 false 512" NXDOMAIN qr,aa,rd 189 0.000359294s
[INFO] 10.244.0.34:46498 - 44037 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000192142s
[INFO] 10.244.0.34:46498 - 42614 "A IN apisix-admin.ingress-apisix.svc.cluster.local.svc.cluster.local. udp 81 false 512" NXDOMAIN qr,aa,rd 174 0.000251383s
[INFO] 10.244.0.34:53686 - 1610 "A IN apisix-admin.ingress-apisix.svc.cluster.local.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000206156s
[INFO] 10.244.0.34:53686 - 2857 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local.cluster.local. udp 77 false 512" NXDOMAIN qr,aa,rd 170 0.000302595s
[INFO] 10.244.0.34:47084 - 9836 "AAAA IN apisix-admin.ingress-apisix.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 156 0.000162789s
[INFO] 10.244.0.34:47084 - 9030 "A IN apisix-admin.ingress-apisix.svc.cluster.local. udp 63 false 512" NOERROR qr,aa,rd 124 0.000264863s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_11_12T18_25_31_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 12 Nov 2023 21:25:27 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 28 Jan 2024 20:42:20 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 28 Jan 2024 20:40:20 +0000   Sun, 12 Nov 2023 21:25:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 28 Jan 2024 20:40:20 +0000   Sun, 12 Nov 2023 21:25:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 28 Jan 2024 20:40:20 +0000   Sun, 12 Nov 2023 21:25:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 28 Jan 2024 20:40:20 +0000   Sun, 12 Nov 2023 21:25:41 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  958802032Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             32674800Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  958802032Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             32674800Ki
  pods:               110
System Info:
  Machine ID:                 2313c5325f114c4b95bb686ae1b6d601
  System UUID:                0df48719-25c2-4520-aafd-406ddfa51298
  Boot ID:                    25685434-c45d-4dc3-a764-908552e2d94b
  Kernel Version:             6.1.0-16-amd64
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (14 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     ner-extractor-svc-7d454cd9bd-l6np9            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         80s
  default                     ner-extractor-svc-7d454cd9bd-lr2rm            0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         80s
  ingress-apisix              apisix-bf7895f94-q4mlp                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         69d
  ingress-apisix              apisix-etcd-0                                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         69d
  ingress-apisix              apisix-etcd-1                                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         69d
  ingress-apisix              apisix-etcd-2                                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         69d
  ingress-apisix              apisix-ingress-controller-78ff6d8549-jk7v8    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         69d
  kube-system                 coredns-5d78c9869d-49fn2                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (0%!)(MISSING)     76d
  kube-system                 etcd-minikube                                 100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         76d
  kube-system                 kube-apiserver-minikube                       250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         76d
  kube-system                 kube-controller-manager-minikube              200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         76d
  kube-system                 kube-proxy-vtfrm                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         76d
  kube-system                 kube-scheduler-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         76d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         76d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (0%!)(MISSING)  170Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 4m13s              kube-proxy       
  Normal  Starting                 39m                kube-proxy       
  Normal  Starting                 69d                kube-proxy       
  Normal  Starting                 69d                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  69d (x8 over 69d)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    69d (x8 over 69d)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     69d (x8 over 69d)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           69d                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 39m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  39m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  39m (x8 over 39m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    39m (x8 over 39m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     39m (x7 over 39m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           38m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeNotReady             4m28s              kubelet          Node minikube status is now: NodeNotReady
  Normal  RegisteredNode           4m1s               node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Jan28 16:48] i915 0000:00:02.0: [drm] *ERROR* Atomic update failure on pipe A (start=415754 end=415755) time 931 us, min 1073, max 1079, scanline start 1069, end 11
[  +4.996911] i915 0000:00:02.0: [drm] *ERROR* Atomic update failure on pipe A (start=416054 end=416055) time 773 us, min 1073, max 1079, scanline start 1052, end 1097
[Jan28 17:17] i915 0000:00:02.0: [drm] *ERROR* Atomic update failure on pipe A (start=520189 end=520190) time 1228 us, min 1073, max 1079, scanline start 1051, end 16
[Jan28 18:03] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PCI0.PEG0.PEGP.BRT6.LCD], AE_NOT_FOUND (20220331/psargs-330)
[  +0.000029] ACPI Error: Aborting method \_SB.PCI0.PEG0.PEGP.BRT6 due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000430] ACPI Error: Aborting method \EV5 due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000428] ACPI Error: Aborting method \SMEE due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000272] ACPI Error: Aborting method \SMIE due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000184] ACPI Error: Aborting method \NEVT due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000181] ACPI Error: Aborting method \_SB.PCI0.LPCB.ECDV._Q66 due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.621950] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PCI0.PEG0.PEGP.BRT6.LCD], AE_NOT_FOUND (20220331/psargs-330)
[  +0.000027] ACPI Error: Aborting method \_SB.PCI0.PEG0.PEGP.BRT6 due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000197] ACPI Error: Aborting method \EV5 due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000152] ACPI Error: Aborting method \SMEE due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000152] ACPI Error: Aborting method \SMIE due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000151] ACPI Error: Aborting method \NEVT due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000160] ACPI Error: Aborting method \_SB.PCI0.LPCB.ECDV._Q66 due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.240895] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PCI0.PEG0.PEGP.BRT6.LCD], AE_NOT_FOUND (20220331/psargs-330)
[  +0.000031] ACPI Error: Aborting method \_SB.PCI0.PEG0.PEGP.BRT6 due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000480] ACPI Error: Aborting method \EV5 due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000479] ACPI Error: Aborting method \SMEE due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000368] ACPI Error: Aborting method \SMIE due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000232] ACPI Error: Aborting method \NEVT due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000240] ACPI Error: Aborting method \_SB.PCI0.LPCB.ECDV._Q66 due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.407004] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PCI0.PEG0.PEGP.BRT6.LCD], AE_NOT_FOUND (20220331/psargs-330)
[  +0.000031] ACPI Error: Aborting method \_SB.PCI0.PEG0.PEGP.BRT6 due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000433] ACPI Error: Aborting method \EV5 due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000296] ACPI Error: Aborting method \SMEE due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000228] ACPI Error: Aborting method \SMIE due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000198] ACPI Error: Aborting method \NEVT due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[  +0.000180] ACPI Error: Aborting method \_SB.PCI0.LPCB.ECDV._Q66 due to previous error (AE_NOT_FOUND) (20220331/psparse-529)
[Jan28 18:13] i915 0000:00:02.0: [drm] *ERROR* Atomic update failure on pipe A (start=723358 end=723359) time 1661 us, min 1073, max 1079, scanline start 1056, end 49
[Jan28 18:41] i915 0000:00:02.0: [drm] *ERROR* Atomic update failure on pipe A (start=822990 end=822991) time 877 us, min 1073, max 1079, scanline start 1024, end 1082
[Jan28 19:05] i915 0000:00:02.0: [drm] *ERROR* Atomic update failure on pipe A (start=909118 end=909119) time 964 us, min 1073, max 1079, scanline start 1017, end 1076
[Jan28 19:18] i915 0000:00:02.0: [drm] *ERROR* Atomic update failure on pipe A (start=958034 end=958035) time 1521 us, min 1073, max 1079, scanline start 1003, end 1097
[Jan28 19:26] i915 0000:00:02.0: [drm] *ERROR* Atomic update failure on pipe A (start=984743 end=984744) time 1741 us, min 1073, max 1079, scanline start 1068, end 72
[Jan28 19:29] i915 0000:00:02.0: [drm] *ERROR* Atomic update failure on pipe A (start=995247 end=995248) time 1027 us, min 1073, max 1079, scanline start 1059, end 7
[Jan28 19:37] i915 0000:00:02.0: [drm] *ERROR* Atomic update failure on pipe A (start=1024657 end=1024658) time 282 us, min 1073, max 1079, scanline start 1064, end 1082
[Jan28 20:31] i915 0000:00:02.0: [drm] *ERROR* Atomic update failure on pipe A (start=1220021 end=1220022) time 666 us, min 1073, max 1079, scanline start 1064, end 1108

* 
* ==> etcd [07031615dc2f] <==
* {"level":"info","ts":"2024-01-28T20:38:41.559Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":30}
{"level":"info","ts":"2024-01-28T20:38:41.560Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-01-28T20:38:41.561Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"47324e080c98137d","local-server-version":"3.5.7","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-01-28T20:38:41.561Z","caller":"etcdserver/server.go:754","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-01-28T20:38:41.561Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/bitnami/etcd/data/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T20:38:41.561Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/bitnami/etcd/data/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T20:38:41.562Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/bitnami/etcd/data/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T20:38:41.562Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"47324e080c98137d switched to configuration voters=(4171443932729916013)"}
{"level":"info","ts":"2024-01-28T20:38:41.563Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"b9ae902a9398fd61","local-member-id":"47324e080c98137d","added-peer-id":"39e3f227d2d33a6d","added-peer-peer-urls":["http://apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.563Z","caller":"rafthttp/peer.go:133","msg":"starting remote peer","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.563Z","caller":"rafthttp/pipeline.go:72","msg":"started HTTP pipelining with remote peer","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.563Z","caller":"rafthttp/stream.go:169","msg":"started stream writer with remote peer","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.565Z","caller":"rafthttp/stream.go:169","msg":"started stream writer with remote peer","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.566Z","caller":"rafthttp/peer.go:137","msg":"started remote peer","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.566Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.566Z","caller":"rafthttp/transport.go:317","msg":"added remote peer","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d","remote-peer-urls":["http://apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.566Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"47324e080c98137d switched to configuration voters=(4171443932729916013 5130248722006414205)"}
{"level":"info","ts":"2024-01-28T20:38:41.566Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"b9ae902a9398fd61","local-member-id":"47324e080c98137d","added-peer-id":"47324e080c98137d","added-peer-peer-urls":["http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.566Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.566Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"47324e080c98137d switched to configuration voters=(4171443932729916013 5130248722006414205 16309538207480531017)"}
{"level":"info","ts":"2024-01-28T20:38:41.566Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"b9ae902a9398fd61","local-member-id":"47324e080c98137d","added-peer-id":"e2571e968b89c849","added-peer-peer-urls":["http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.566Z","caller":"rafthttp/peer.go:133","msg":"starting remote peer","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.566Z","caller":"rafthttp/pipeline.go:72","msg":"started HTTP pipelining with remote peer","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.567Z","caller":"rafthttp/stream.go:169","msg":"started stream writer with remote peer","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.567Z","caller":"rafthttp/peer.go:137","msg":"started remote peer","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.567Z","caller":"rafthttp/stream.go:169","msg":"started stream writer with remote peer","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"rafthttp/transport.go:317","msg":"added remote peer","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849","remote-peer-urls":["http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"b9ae902a9398fd61","local-member-id":"47324e080c98137d","cluster-version":"3.0"}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.0"}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"47324e080c98137d","initial-advertise-peer-urls":["http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"],"listen-peer-urls":["http://0.0.0.0:2380"],"advertise-client-urls":["http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379","http://apisix-etcd.ingress-apisix.svc.cluster.local:2379"],"listen-client-urls":["http://0.0.0.0:2379"],"listen-metrics-urls":[]}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"[::]:2380"}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"membership/cluster.go:576","msg":"updated cluster version","cluster-id":"b9ae902a9398fd61","local-member-id":"47324e080c98137d","from":"3.0","to":"3.5"}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"[::]:2380"}
{"level":"info","ts":"2024-01-28T20:38:41.571Z","caller":"rafthttp/peer_status.go:53","msg":"peer became active","peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.571Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.571Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/peer_status.go:53","msg":"peer became active","peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"47324e080c98137d","to":"39e3f227d2d33a6d","stream-type":"stream Message"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"47324e080c98137d","to":"e2571e968b89c849","stream-type":"stream MsgApp v2"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"47324e080c98137d","to":"e2571e968b89c849","stream-type":"stream Message"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.575Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"47324e080c98137d","to":"39e3f227d2d33a6d","stream-type":"stream MsgApp v2"}
{"level":"info","ts":"2024-01-28T20:38:41.575Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.612Z","caller":"etcdserver/server.go:777","msg":"initialized peer connections; fast-forwarding election ticks","local-member-id":"47324e080c98137d","forward-ticks":8,"forward-duration":"800ms","election-ticks":10,"election-timeout":"1s","active-remote-members":2}
{"level":"info","ts":"2024-01-28T20:38:41.760Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"47324e080c98137d [logterm: 7, index: 1198, vote: 0] cast MsgPreVote for 39e3f227d2d33a6d [logterm: 7, index: 1198] at term 7"}
{"level":"info","ts":"2024-01-28T20:38:41.763Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"47324e080c98137d [term: 7] received a MsgVote message with higher term from 39e3f227d2d33a6d [term: 8]"}
{"level":"info","ts":"2024-01-28T20:38:41.763Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"47324e080c98137d became follower at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.763Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"47324e080c98137d [logterm: 7, index: 1198, vote: 0] cast MsgVote for 39e3f227d2d33a6d [logterm: 7, index: 1198] at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.765Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 47324e080c98137d elected leader 39e3f227d2d33a6d at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.771Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"47324e080c98137d","local-member-attributes":"{Name:apisix-etcd-1 ClientURLs:[http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379 http://apisix-etcd.ingress-apisix.svc.cluster.local:2379]}","request-path":"/0/members/47324e080c98137d/attributes","cluster-id":"b9ae902a9398fd61","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-28T20:38:41.771Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-28T20:38:41.771Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-28T20:38:41.771Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-28T20:38:41.775Z","caller":"embed/serve.go:146","msg":"serving client traffic insecurely; this is strongly discouraged!","address":"[::]:2379"}

* 
* ==> etcd [0a2352ad1b6f] <==
* {"level":"info","ts":"2024-01-28T20:38:41.560Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":30}
{"level":"info","ts":"2024-01-28T20:38:41.561Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-01-28T20:38:41.563Z","caller":"etcdserver/server.go:854","msg":"starting etcd server","local-member-id":"e2571e968b89c849","local-server-version":"3.5.7","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-01-28T20:38:41.563Z","caller":"etcdserver/server.go:754","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-01-28T20:38:41.563Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/bitnami/etcd/data/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T20:38:41.563Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/bitnami/etcd/data/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T20:38:41.563Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/bitnami/etcd/data/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T20:38:41.563Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 switched to configuration voters=(4171443932729916013)"}
{"level":"info","ts":"2024-01-28T20:38:41.564Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"b9ae902a9398fd61","local-member-id":"e2571e968b89c849","added-peer-id":"39e3f227d2d33a6d","added-peer-peer-urls":["http://apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.564Z","caller":"rafthttp/peer.go:133","msg":"starting remote peer","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.564Z","caller":"rafthttp/pipeline.go:72","msg":"started HTTP pipelining with remote peer","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.566Z","caller":"rafthttp/stream.go:169","msg":"started stream writer with remote peer","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.566Z","caller":"rafthttp/stream.go:169","msg":"started stream writer with remote peer","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"rafthttp/peer.go:137","msg":"started remote peer","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"rafthttp/transport.go:317","msg":"added remote peer","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d","remote-peer-urls":["http://apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 switched to configuration voters=(4171443932729916013 5130248722006414205)"}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"b9ae902a9398fd61","local-member-id":"e2571e968b89c849","added-peer-id":"47324e080c98137d","added-peer-peer-urls":["http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.568Z","caller":"rafthttp/peer.go:133","msg":"starting remote peer","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"rafthttp/pipeline.go:72","msg":"started HTTP pipelining with remote peer","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"[::]:2380"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"[::]:2380"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"rafthttp/stream.go:169","msg":"started stream writer with remote peer","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"rafthttp/stream.go:169","msg":"started stream writer with remote peer","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"e2571e968b89c849","initial-advertise-peer-urls":["http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"],"listen-peer-urls":["http://0.0.0.0:2380"],"advertise-client-urls":["http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379","http://apisix-etcd.ingress-apisix.svc.cluster.local:2379"],"listen-client-urls":["http://0.0.0.0:2379"],"listen-metrics-urls":[]}
{"level":"info","ts":"2024-01-28T20:38:41.570Z","caller":"rafthttp/peer.go:137","msg":"started remote peer","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.570Z","caller":"rafthttp/transport.go:317","msg":"added remote peer","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d","remote-peer-urls":["http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.570Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.570Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 switched to configuration voters=(4171443932729916013 5130248722006414205 16309538207480531017)"}
{"level":"info","ts":"2024-01-28T20:38:41.570Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"b9ae902a9398fd61","local-member-id":"e2571e968b89c849","added-peer-id":"e2571e968b89c849","added-peer-peer-urls":["http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.570Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"b9ae902a9398fd61","local-member-id":"e2571e968b89c849","cluster-version":"3.0"}
{"level":"info","ts":"2024-01-28T20:38:41.570Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.0"}
{"level":"info","ts":"2024-01-28T20:38:41.570Z","caller":"membership/cluster.go:576","msg":"updated cluster version","cluster-id":"b9ae902a9398fd61","local-member-id":"e2571e968b89c849","from":"3.0","to":"3.5"}
{"level":"info","ts":"2024-01-28T20:38:41.571Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"e2571e968b89c849","to":"47324e080c98137d","stream-type":"stream Message"}
{"level":"info","ts":"2024-01-28T20:38:41.571Z","caller":"rafthttp/peer_status.go:53","msg":"peer became active","peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.570Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.572Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"e2571e968b89c849","to":"47324e080c98137d","stream-type":"stream MsgApp v2"}
{"level":"info","ts":"2024-01-28T20:38:41.572Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.572Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/peer_status.go:53","msg":"peer became active","peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.575Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"e2571e968b89c849","to":"39e3f227d2d33a6d","stream-type":"stream Message"}
{"level":"info","ts":"2024-01-28T20:38:41.575Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.575Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"e2571e968b89c849","to":"39e3f227d2d33a6d","stream-type":"stream MsgApp v2"}
{"level":"info","ts":"2024-01-28T20:38:41.575Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:38:41.613Z","caller":"etcdserver/server.go:777","msg":"initialized peer connections; fast-forwarding election ticks","local-member-id":"e2571e968b89c849","forward-ticks":8,"forward-duration":"800ms","election-ticks":10,"election-timeout":"1s","active-remote-members":2}
{"level":"info","ts":"2024-01-28T20:38:41.760Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 [logterm: 7, index: 1198, vote: 39e3f227d2d33a6d] cast MsgPreVote for 39e3f227d2d33a6d [logterm: 7, index: 1198] at term 7"}
{"level":"info","ts":"2024-01-28T20:38:41.763Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 [term: 7] received a MsgVote message with higher term from 39e3f227d2d33a6d [term: 8]"}
{"level":"info","ts":"2024-01-28T20:38:41.764Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 became follower at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.764Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 [logterm: 7, index: 1198, vote: 0] cast MsgVote for 39e3f227d2d33a6d [logterm: 7, index: 1198] at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.765Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: e2571e968b89c849 elected leader 39e3f227d2d33a6d at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.769Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-28T20:38:41.769Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"e2571e968b89c849","local-member-attributes":"{Name:apisix-etcd-0 ClientURLs:[http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379 http://apisix-etcd.ingress-apisix.svc.cluster.local:2379]}","request-path":"/0/members/e2571e968b89c849/attributes","cluster-id":"b9ae902a9398fd61","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-28T20:38:41.770Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-28T20:38:41.770Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-28T20:38:41.773Z","caller":"embed/serve.go:146","msg":"serving client traffic insecurely; this is strongly discouraged!","address":"[::]:2379"}

* 
* ==> etcd [337414b98f6c] <==
* {"level":"info","ts":"2024-01-28T20:04:39.187Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d is starting a new election at term 6"}
{"level":"info","ts":"2024-01-28T20:04:39.187Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d became pre-candidate at term 6"}
{"level":"info","ts":"2024-01-28T20:04:39.187Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d received MsgPreVoteResp from 39e3f227d2d33a6d at term 6"}
{"level":"info","ts":"2024-01-28T20:04:39.187Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d [logterm: 5, index: 387] sent MsgPreVote request to 47324e080c98137d at term 6"}
{"level":"info","ts":"2024-01-28T20:04:39.187Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d [logterm: 5, index: 387] sent MsgPreVote request to e2571e968b89c849 at term 6"}
{"level":"info","ts":"2024-01-28T20:04:39.188Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d received MsgPreVoteResp from e2571e968b89c849 at term 6"}
{"level":"info","ts":"2024-01-28T20:04:39.188Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d has received 2 MsgPreVoteResp votes and 0 vote rejections"}
{"level":"info","ts":"2024-01-28T20:04:39.188Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d became candidate at term 7"}
{"level":"info","ts":"2024-01-28T20:04:39.188Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d received MsgVoteResp from 39e3f227d2d33a6d at term 7"}
{"level":"info","ts":"2024-01-28T20:04:39.188Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d [logterm: 5, index: 387] sent MsgVote request to 47324e080c98137d at term 7"}
{"level":"info","ts":"2024-01-28T20:04:39.188Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d [logterm: 5, index: 387] sent MsgVote request to e2571e968b89c849 at term 7"}
{"level":"info","ts":"2024-01-28T20:04:39.191Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d received MsgVoteResp from e2571e968b89c849 at term 7"}
{"level":"info","ts":"2024-01-28T20:04:39.191Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d has received 2 MsgVoteResp votes and 0 vote rejections"}
{"level":"info","ts":"2024-01-28T20:04:39.191Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d became leader at term 7"}
{"level":"info","ts":"2024-01-28T20:04:39.191Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 39e3f227d2d33a6d elected leader 39e3f227d2d33a6d at term 7"}
{"level":"warn","ts":"2024-01-28T20:04:39.198Z","caller":"etcdserver/cluster_util.go:288","msg":"failed to reach the peer URL","address":"http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380/version","remote-member-id":"47324e080c98137d","error":"Get \"http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380/version\": dial tcp 10.244.0.23:2380: connect: connection refused"}
{"level":"warn","ts":"2024-01-28T20:04:39.198Z","caller":"etcdserver/cluster_util.go:155","msg":"failed to get version","remote-member-id":"47324e080c98137d","error":"Get \"http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380/version\": dial tcp 10.244.0.23:2380: connect: connection refused"}
{"level":"info","ts":"2024-01-28T20:04:39.198Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-28T20:04:39.198Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"39e3f227d2d33a6d","local-member-attributes":"{Name:apisix-etcd-2 ClientURLs:[http://apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379 http://apisix-etcd.ingress-apisix.svc.cluster.local:2379]}","request-path":"/0/members/39e3f227d2d33a6d/attributes","cluster-id":"b9ae902a9398fd61","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-28T20:04:39.199Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-28T20:04:39.199Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-28T20:04:39.202Z","caller":"embed/serve.go:146","msg":"serving client traffic insecurely; this is strongly discouraged!","address":"[::]:2379"}
{"level":"info","ts":"2024-01-28T20:04:39.927Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"39e3f227d2d33a6d","to":"47324e080c98137d","stream-type":"stream Message"}
{"level":"info","ts":"2024-01-28T20:04:39.927Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"39e3f227d2d33a6d","to":"47324e080c98137d","stream-type":"stream MsgApp v2"}
{"level":"info","ts":"2024-01-28T20:04:39.927Z","caller":"rafthttp/peer_status.go:53","msg":"peer became active","peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:04:39.927Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream Message","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:04:39.928Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:04:39.963Z","caller":"etcdserver/server.go:777","msg":"initialized peer connections; fast-forwarding election ticks","local-member-id":"39e3f227d2d33a6d","forward-ticks":8,"forward-duration":"800ms","election-ticks":10,"election-timeout":"1s","active-remote-members":2}
{"level":"info","ts":"2024-01-28T20:04:40.002Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:04:40.002Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"warn","ts":"2024-01-28T20:37:56.076Z","caller":"rafthttp/stream.go:421","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849","error":"EOF"}
{"level":"warn","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/stream.go:421","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849","error":"EOF"}
{"level":"warn","ts":"2024-01-28T20:37:56.079Z","caller":"rafthttp/peer_status.go:66","msg":"peer became inactive (message send to peer failed)","peer-id":"e2571e968b89c849","error":"failed to dial e2571e968b89c849 on stream Message (peer e2571e968b89c849 failed to find local node 39e3f227d2d33a6d)"}
{"level":"warn","ts":"2024-01-28T20:37:56.092Z","caller":"rafthttp/stream.go:421","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d","error":"EOF"}
{"level":"warn","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/stream.go:421","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d","error":"EOF"}
{"level":"warn","ts":"2024-01-28T20:37:56.095Z","caller":"rafthttp/peer_status.go:66","msg":"peer became inactive (message send to peer failed)","peer-id":"47324e080c98137d","error":"failed to dial 47324e080c98137d on stream Message (peer 47324e080c98137d failed to find local node 39e3f227d2d33a6d)"}
{"level":"info","ts":"2024-01-28T20:37:56.105Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-01-28T20:37:56.105Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"apisix-etcd-2","data-dir":"/bitnami/etcd/data","advertise-peer-urls":["http://apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"],"advertise-client-urls":["http://apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379","http://apisix-etcd.ingress-apisix.svc.cluster.local:2379"]}
{"level":"warn","ts":"2024-01-28T20:37:56.107Z","caller":"etcdserver/server.go:1504","msg":"leadership transfer failed","local-member-id":"39e3f227d2d33a6d","error":"etcdserver: unhealthy cluster"}
{"level":"info","ts":"2024-01-28T20:37:56.107Z","caller":"rafthttp/peer.go:330","msg":"stopping remote peer","remote-peer-id":"e2571e968b89c849"}
{"level":"warn","ts":"2024-01-28T20:37:56.107Z","caller":"rafthttp/stream.go:286","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.107Z","caller":"rafthttp/stream.go:294","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"e2571e968b89c849"}
{"level":"warn","ts":"2024-01-28T20:37:56.107Z","caller":"rafthttp/stream.go:286","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.107Z","caller":"rafthttp/stream.go:294","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.107Z","caller":"rafthttp/pipeline.go:85","msg":"stopped HTTP pipelining with remote peer","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.107Z","caller":"rafthttp/stream.go:442","msg":"stopped stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.108Z","caller":"rafthttp/stream.go:442","msg":"stopped stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.108Z","caller":"rafthttp/peer.go:335","msg":"stopped remote peer","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.108Z","caller":"rafthttp/peer.go:330","msg":"stopping remote peer","remote-peer-id":"47324e080c98137d"}
{"level":"warn","ts":"2024-01-28T20:37:56.108Z","caller":"rafthttp/stream.go:286","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:37:56.108Z","caller":"rafthttp/stream.go:294","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"47324e080c98137d"}
{"level":"warn","ts":"2024-01-28T20:37:56.108Z","caller":"rafthttp/stream.go:286","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:37:56.108Z","caller":"rafthttp/stream.go:294","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:37:56.108Z","caller":"rafthttp/pipeline.go:85","msg":"stopped HTTP pipelining with remote peer","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:37:56.108Z","caller":"rafthttp/stream.go:442","msg":"stopped stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:37:56.108Z","caller":"rafthttp/stream.go:442","msg":"stopped stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:37:56.108Z","caller":"rafthttp/peer.go:335","msg":"stopped remote peer","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:37:56.110Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"[::]:2380"}
{"level":"info","ts":"2024-01-28T20:37:56.112Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"[::]:2380"}
{"level":"info","ts":"2024-01-28T20:37:56.113Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"apisix-etcd-2","data-dir":"/bitnami/etcd/data","advertise-peer-urls":["http://apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"],"advertise-client-urls":["http://apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379","http://apisix-etcd.ingress-apisix.svc.cluster.local:2379"]}

* 
* ==> etcd [7fc52855b830] <==
* {"level":"info","ts":"2024-01-28T20:04:38.001Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:04:38.040Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:04:38.040Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:04:39.125Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 is starting a new election at term 5"}
{"level":"info","ts":"2024-01-28T20:04:39.125Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 became pre-candidate at term 5"}
{"level":"info","ts":"2024-01-28T20:04:39.125Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 received MsgPreVoteResp from e2571e968b89c849 at term 5"}
{"level":"info","ts":"2024-01-28T20:04:39.125Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 [logterm: 5, index: 387] sent MsgPreVote request to 39e3f227d2d33a6d at term 5"}
{"level":"info","ts":"2024-01-28T20:04:39.125Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 [logterm: 5, index: 387] sent MsgPreVote request to 47324e080c98137d at term 5"}
{"level":"info","ts":"2024-01-28T20:04:39.126Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 [term: 5] received a MsgPreVoteResp message with higher term from 39e3f227d2d33a6d [term: 6]"}
{"level":"info","ts":"2024-01-28T20:04:39.126Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 became follower at term 6"}
{"level":"info","ts":"2024-01-28T20:04:39.188Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 [logterm: 5, index: 387, vote: 0] cast MsgPreVote for 39e3f227d2d33a6d [logterm: 5, index: 387] at term 6"}
{"level":"info","ts":"2024-01-28T20:04:39.190Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 [term: 6] received a MsgVote message with higher term from 39e3f227d2d33a6d [term: 7]"}
{"level":"info","ts":"2024-01-28T20:04:39.190Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 became follower at term 7"}
{"level":"info","ts":"2024-01-28T20:04:39.190Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"e2571e968b89c849 [logterm: 5, index: 387, vote: 0] cast MsgVote for 39e3f227d2d33a6d [logterm: 5, index: 387] at term 7"}
{"level":"info","ts":"2024-01-28T20:04:39.192Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: e2571e968b89c849 elected leader 39e3f227d2d33a6d at term 7"}
{"level":"info","ts":"2024-01-28T20:04:39.199Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"e2571e968b89c849","local-member-attributes":"{Name:apisix-etcd-0 ClientURLs:[http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379 http://apisix-etcd.ingress-apisix.svc.cluster.local:2379]}","request-path":"/0/members/e2571e968b89c849/attributes","cluster-id":"b9ae902a9398fd61","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-28T20:04:39.199Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-28T20:04:39.199Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-28T20:04:39.199Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-28T20:04:39.202Z","caller":"embed/serve.go:146","msg":"serving client traffic insecurely; this is strongly discouraged!","address":"[::]:2379"}
{"level":"info","ts":"2024-01-28T20:04:39.929Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"e2571e968b89c849","to":"47324e080c98137d","stream-type":"stream MsgApp v2"}
{"level":"info","ts":"2024-01-28T20:04:39.929Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"e2571e968b89c849","to":"47324e080c98137d","stream-type":"stream Message"}
{"level":"info","ts":"2024-01-28T20:04:39.929Z","caller":"rafthttp/peer_status.go:53","msg":"peer became active","peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:04:39.929Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:04:39.929Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:04:39.941Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:04:39.941Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:04:39.952Z","caller":"etcdserver/server.go:777","msg":"initialized peer connections; fast-forwarding election ticks","local-member-id":"e2571e968b89c849","forward-ticks":8,"forward-duration":"800ms","election-ticks":10,"election-timeout":"1s","active-remote-members":2}
{"level":"info","ts":"2024-01-28T20:37:56.073Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-01-28T20:37:56.073Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"apisix-etcd-0","data-dir":"/bitnami/etcd/data","advertise-peer-urls":["http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"],"advertise-client-urls":["http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379","http://apisix-etcd.ingress-apisix.svc.cluster.local:2379"]}
{"level":"info","ts":"2024-01-28T20:37:56.075Z","caller":"etcdserver/server.go:1456","msg":"skipped leadership transfer; local server is not leader","local-member-id":"e2571e968b89c849","current-leader-member-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:37:56.075Z","caller":"rafthttp/peer.go:330","msg":"stopping remote peer","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"warn","ts":"2024-01-28T20:37:56.076Z","caller":"rafthttp/stream.go:286","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:37:56.076Z","caller":"rafthttp/stream.go:294","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"warn","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/stream.go:286","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/stream.go:294","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/pipeline.go:85","msg":"stopped HTTP pipelining with remote peer","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"warn","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/stream.go:421","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d","error":"context canceled"}
{"level":"warn","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/peer_status.go:66","msg":"peer became inactive (message send to peer failed)","peer-id":"39e3f227d2d33a6d","error":"failed to read 39e3f227d2d33a6d on stream MsgApp v2 (context canceled)"}
{"level":"info","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/stream.go:442","msg":"stopped stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"warn","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/stream.go:421","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d","error":"context canceled"}
{"level":"info","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/stream.go:442","msg":"stopped stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/peer.go:335","msg":"stopped remote peer","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/peer.go:330","msg":"stopping remote peer","remote-peer-id":"47324e080c98137d"}
{"level":"warn","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/stream.go:286","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:37:56.077Z","caller":"rafthttp/stream.go:294","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"47324e080c98137d"}
{"level":"warn","ts":"2024-01-28T20:37:56.078Z","caller":"rafthttp/stream.go:286","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:37:56.078Z","caller":"rafthttp/stream.go:294","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:37:56.078Z","caller":"rafthttp/pipeline.go:85","msg":"stopped HTTP pipelining with remote peer","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"warn","ts":"2024-01-28T20:37:56.078Z","caller":"rafthttp/stream.go:421","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d","error":"context canceled"}
{"level":"warn","ts":"2024-01-28T20:37:56.078Z","caller":"rafthttp/peer_status.go:66","msg":"peer became inactive (message send to peer failed)","peer-id":"47324e080c98137d","error":"failed to read 47324e080c98137d on stream MsgApp v2 (context canceled)"}
{"level":"info","ts":"2024-01-28T20:37:56.078Z","caller":"rafthttp/stream.go:442","msg":"stopped stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"warn","ts":"2024-01-28T20:37:56.078Z","caller":"rafthttp/stream.go:421","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d","error":"context canceled"}
{"level":"info","ts":"2024-01-28T20:37:56.078Z","caller":"rafthttp/stream.go:442","msg":"stopped stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"e2571e968b89c849","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:37:56.078Z","caller":"rafthttp/peer.go:335","msg":"stopped remote peer","remote-peer-id":"47324e080c98137d"}
{"level":"warn","ts":"2024-01-28T20:37:56.079Z","caller":"rafthttp/http.go:413","msg":"failed to find remote peer in cluster","local-member-id":"e2571e968b89c849","remote-peer-id-stream-handler":"e2571e968b89c849","remote-peer-id-from":"39e3f227d2d33a6d","cluster-id":"b9ae902a9398fd61"}
{"level":"warn","ts":"2024-01-28T20:37:56.079Z","caller":"rafthttp/http.go:413","msg":"failed to find remote peer in cluster","local-member-id":"e2571e968b89c849","remote-peer-id-stream-handler":"e2571e968b89c849","remote-peer-id-from":"39e3f227d2d33a6d","cluster-id":"b9ae902a9398fd61"}
{"level":"info","ts":"2024-01-28T20:37:56.082Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"[::]:2380"}
{"level":"info","ts":"2024-01-28T20:37:56.155Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"[::]:2380"}
{"level":"info","ts":"2024-01-28T20:37:56.155Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"apisix-etcd-0","data-dir":"/bitnami/etcd/data","advertise-peer-urls":["http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"],"advertise-client-urls":["http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379","http://apisix-etcd.ingress-apisix.svc.cluster.local:2379"]}

* 
* ==> etcd [9486f545da69] <==
* {"level":"info","ts":"2024-01-28T20:03:18.836Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 4"}
{"level":"info","ts":"2024-01-28T20:03:18.836Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 4, commit: 406930, applied: 400040, lastindex: 406930, lastterm: 4]"}
{"level":"info","ts":"2024-01-28T20:03:18.836Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-28T20:03:18.836Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-01-28T20:03:18.836Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-01-28T20:03:18.837Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-01-28T20:03:18.838Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":324019}
{"level":"info","ts":"2024-01-28T20:03:18.840Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":324599}
{"level":"info","ts":"2024-01-28T20:03:18.841Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-01-28T20:03:18.842Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-01-28T20:03:18.843Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-01-28T20:03:18.843Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-28T20:03:18.843Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-01-28T20:03:18.843Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T20:03:18.843Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T20:03:18.843Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T20:03:18.847Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-28T20:03:18.848Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-28T20:03:18.848Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-28T20:03:18.848Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-01-28T20:03:18.848Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-01-28T20:03:19.337Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 4"}
{"level":"info","ts":"2024-01-28T20:03:19.337Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 4"}
{"level":"info","ts":"2024-01-28T20:03:19.337Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-01-28T20:03:19.337Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 5"}
{"level":"info","ts":"2024-01-28T20:03:19.337Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-01-28T20:03:19.337Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 5"}
{"level":"info","ts":"2024-01-28T20:03:19.337Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-01-28T20:03:19.339Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-28T20:03:19.340Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-28T20:03:19.340Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-28T20:03:19.340Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-28T20:03:19.340Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-28T20:03:19.343Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-01-28T20:03:19.343Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-01-28T20:13:19.373Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":325224}
{"level":"info","ts":"2024-01-28T20:13:19.389Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":325224,"took":"15.906701ms","hash":3533100640}
{"level":"info","ts":"2024-01-28T20:13:19.389Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3533100640,"revision":325224,"compact-revision":324019}
{"level":"info","ts":"2024-01-28T20:18:19.378Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":325640}
{"level":"info","ts":"2024-01-28T20:18:19.381Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":325640,"took":"2.062725ms","hash":1974207429}
{"level":"info","ts":"2024-01-28T20:18:19.381Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1974207429,"revision":325640,"compact-revision":325224}
{"level":"info","ts":"2024-01-28T20:23:19.383Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":326037}
{"level":"info","ts":"2024-01-28T20:23:19.385Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":326037,"took":"894.557¬µs","hash":2328270450}
{"level":"info","ts":"2024-01-28T20:23:19.385Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2328270450,"revision":326037,"compact-revision":325640}
{"level":"info","ts":"2024-01-28T20:28:19.389Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":326431}
{"level":"info","ts":"2024-01-28T20:28:19.391Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":326431,"took":"1.628474ms","hash":1569102418}
{"level":"info","ts":"2024-01-28T20:28:19.391Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1569102418,"revision":326431,"compact-revision":326037}
{"level":"info","ts":"2024-01-28T20:33:07.180Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":410041,"local-member-snapshot-index":400040,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-01-28T20:33:07.183Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":410041}
{"level":"info","ts":"2024-01-28T20:33:07.184Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":405041}
{"level":"info","ts":"2024-01-28T20:33:18.885Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000003-0000000000057e64.snap"}
{"level":"info","ts":"2024-01-28T20:33:19.395Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":326847}
{"level":"info","ts":"2024-01-28T20:33:19.396Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":326847,"took":"1.289171ms","hash":2456354844}
{"level":"info","ts":"2024-01-28T20:33:19.396Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2456354844,"revision":326847,"compact-revision":326431}
{"level":"info","ts":"2024-01-28T20:37:56.079Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-01-28T20:37:56.079Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-01-28T20:37:56.120Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-01-28T20:37:56.123Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-28T20:37:56.124Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-28T20:37:56.124Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [989b491f8982] <==
* {"level":"info","ts":"2024-01-28T20:38:11.821Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-01-28T20:38:11.822Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2024-01-28T20:38:11.822Z","caller":"embed/etcd.go:124","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-01-28T20:38:11.822Z","caller":"embed/etcd.go:484","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-28T20:38:11.823Z","caller":"embed/etcd.go:132","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-01-28T20:38:11.823Z","caller":"embed/etcd.go:306","msg":"starting an etcd server","etcd-version":"3.5.7","git-sha":"215b53cf3","go-version":"go1.17.13","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-01-28T20:38:11.827Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"3.536322ms"}
{"level":"info","ts":"2024-01-28T20:38:12.734Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":410041,"snapshot-size":"8.2 kB"}
{"level":"info","ts":"2024-01-28T20:38:12.734Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":5046272,"backend-size":"5.0 MB","backend-size-in-use-bytes":3178496,"backend-size-in-use":"3.2 MB"}
{"level":"info","ts":"2024-01-28T20:38:12.785Z","caller":"etcdserver/raft.go:529","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":410541}
{"level":"info","ts":"2024-01-28T20:38:12.785Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-01-28T20:38:12.785Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 5"}
{"level":"info","ts":"2024-01-28T20:38:12.785Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 5, commit: 410541, applied: 410041, lastindex: 410541, lastterm: 5]"}
{"level":"info","ts":"2024-01-28T20:38:12.786Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-28T20:38:12.786Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-01-28T20:38:12.786Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-01-28T20:38:12.786Z","caller":"auth/store.go:1234","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-01-28T20:38:12.787Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":326847}
{"level":"info","ts":"2024-01-28T20:38:12.791Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":327751}
{"level":"info","ts":"2024-01-28T20:38:12.793Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-01-28T20:38:12.794Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-01-28T20:38:12.795Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-01-28T20:38:12.795Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.7","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-01-28T20:38:12.795Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-01-28T20:38:12.796Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T20:38:12.796Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T20:38:12.796Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-01-28T20:38:12.798Z","caller":"embed/etcd.go:687","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-01-28T20:38:12.798Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-28T20:38:12.798Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-01-28T20:38:12.798Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-01-28T20:38:12.798Z","caller":"embed/etcd.go:762","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-01-28T20:38:13.386Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 5"}
{"level":"info","ts":"2024-01-28T20:38:13.386Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 5"}
{"level":"info","ts":"2024-01-28T20:38:13.386Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-01-28T20:38:13.386Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 6"}
{"level":"info","ts":"2024-01-28T20:38:13.386Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-01-28T20:38:13.386Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2024-01-28T20:38:13.386Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-01-28T20:38:13.388Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-28T20:38:13.389Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-28T20:38:13.389Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-28T20:38:13.389Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-28T20:38:13.389Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-28T20:38:13.390Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-01-28T20:38:13.390Z","caller":"embed/serve.go:198","msg":"serving client traffic securely","address":"192.168.49.2:2379"}

* 
* ==> etcd [b00b6187d796] <==
* {"level":"info","ts":"2024-01-28T20:38:41.567Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"b9ae902a9398fd61","local-member-id":"39e3f227d2d33a6d","added-peer-id":"47324e080c98137d","added-peer-peer-urls":["http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.567Z","caller":"rafthttp/peer.go:133","msg":"starting remote peer","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.567Z","caller":"rafthttp/pipeline.go:72","msg":"started HTTP pipelining with remote peer","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"rafthttp/stream.go:169","msg":"started stream writer with remote peer","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"rafthttp/stream.go:169","msg":"started stream writer with remote peer","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"rafthttp/peer.go:137","msg":"started remote peer","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"rafthttp/transport.go:317","msg":"added remote peer","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d","remote-peer-urls":["http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d switched to configuration voters=(4171443932729916013 5130248722006414205 16309538207480531017)"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"b9ae902a9398fd61","local-member-id":"39e3f227d2d33a6d","added-peer-id":"e2571e968b89c849","added-peer-peer-urls":["http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"rafthttp/peer.go:133","msg":"starting remote peer","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.569Z","caller":"rafthttp/pipeline.go:72","msg":"started HTTP pipelining with remote peer","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.571Z","caller":"rafthttp/stream.go:169","msg":"started stream writer with remote peer","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.571Z","caller":"rafthttp/stream.go:169","msg":"started stream writer with remote peer","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.571Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.572Z","caller":"rafthttp/peer.go:137","msg":"started remote peer","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.573Z","caller":"embed/etcd.go:275","msg":"now serving peer/client/metrics","local-member-id":"39e3f227d2d33a6d","initial-advertise-peer-urls":["http://apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"],"listen-peer-urls":["http://0.0.0.0:2380"],"advertise-client-urls":["http://apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379","http://apisix-etcd.ingress-apisix.svc.cluster.local:2379"],"listen-client-urls":["http://0.0.0.0:2379"],"listen-metrics-urls":[]}
{"level":"info","ts":"2024-01-28T20:38:41.573Z","caller":"embed/etcd.go:586","msg":"serving peer traffic","address":"[::]:2380"}
{"level":"info","ts":"2024-01-28T20:38:41.573Z","caller":"embed/etcd.go:558","msg":"cmux::serve","address":"[::]:2380"}
{"level":"info","ts":"2024-01-28T20:38:41.573Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.573Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/transport.go:317","msg":"added remote peer","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849","remote-peer-urls":["http://apisix-etcd-0.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"]}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"39e3f227d2d33a6d","to":"47324e080c98137d","stream-type":"stream Message"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/peer_status.go:53","msg":"peer became active","peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"b9ae902a9398fd61","local-member-id":"39e3f227d2d33a6d","cluster-version":"3.0"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream Message","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.0"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"39e3f227d2d33a6d","to":"e2571e968b89c849","stream-type":"stream Message"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"39e3f227d2d33a6d","to":"e2571e968b89c849","stream-type":"stream MsgApp v2"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/peer_status.go:53","msg":"peer became active","peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream Message","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"39e3f227d2d33a6d","to":"47324e080c98137d","stream-type":"stream MsgApp v2"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"membership/cluster.go:576","msg":"updated cluster version","cluster-id":"b9ae902a9398fd61","local-member-id":"39e3f227d2d33a6d","from":"3.0","to":"3.5"}
{"level":"info","ts":"2024-01-28T20:38:41.574Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.575Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.575Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:38:41.575Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"39e3f227d2d33a6d","remote-peer-id":"47324e080c98137d"}
{"level":"info","ts":"2024-01-28T20:38:41.615Z","caller":"etcdserver/server.go:777","msg":"initialized peer connections; fast-forwarding election ticks","local-member-id":"39e3f227d2d33a6d","forward-ticks":8,"forward-duration":"800ms","election-ticks":10,"election-timeout":"1s","active-remote-members":2}
{"level":"info","ts":"2024-01-28T20:38:41.759Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d is starting a new election at term 7"}
{"level":"info","ts":"2024-01-28T20:38:41.760Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d became pre-candidate at term 7"}
{"level":"info","ts":"2024-01-28T20:38:41.760Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d received MsgPreVoteResp from 39e3f227d2d33a6d at term 7"}
{"level":"info","ts":"2024-01-28T20:38:41.760Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d [logterm: 7, index: 1198] sent MsgPreVote request to 47324e080c98137d at term 7"}
{"level":"info","ts":"2024-01-28T20:38:41.760Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d [logterm: 7, index: 1198] sent MsgPreVote request to e2571e968b89c849 at term 7"}
{"level":"info","ts":"2024-01-28T20:38:41.761Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d received MsgPreVoteResp from e2571e968b89c849 at term 7"}
{"level":"info","ts":"2024-01-28T20:38:41.761Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d has received 2 MsgPreVoteResp votes and 0 vote rejections"}
{"level":"info","ts":"2024-01-28T20:38:41.761Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d became candidate at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.761Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d received MsgVoteResp from 39e3f227d2d33a6d at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.761Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d [logterm: 7, index: 1198] sent MsgVote request to 47324e080c98137d at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.761Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d [logterm: 7, index: 1198] sent MsgVote request to e2571e968b89c849 at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.764Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d received MsgVoteResp from 47324e080c98137d at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.764Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d has received 2 MsgVoteResp votes and 0 vote rejections"}
{"level":"info","ts":"2024-01-28T20:38:41.764Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"39e3f227d2d33a6d became leader at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.764Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 39e3f227d2d33a6d elected leader 39e3f227d2d33a6d at term 8"}
{"level":"info","ts":"2024-01-28T20:38:41.769Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-28T20:38:41.769Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"39e3f227d2d33a6d","local-member-attributes":"{Name:apisix-etcd-2 ClientURLs:[http://apisix-etcd-2.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379 http://apisix-etcd.ingress-apisix.svc.cluster.local:2379]}","request-path":"/0/members/39e3f227d2d33a6d/attributes","cluster-id":"b9ae902a9398fd61","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-28T20:38:41.770Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-28T20:38:41.770Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-28T20:38:41.777Z","caller":"embed/serve.go:146","msg":"serving client traffic insecurely; this is strongly discouraged!","address":"[::]:2379"}

* 
* ==> etcd [dc4465e1b455] <==
* {"level":"info","ts":"2024-01-28T20:04:39.927Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:04:39.927Z","caller":"rafthttp/stream.go:395","msg":"started stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:04:39.927Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"b9ae902a9398fd61","local-member-id":"47324e080c98137d","cluster-version":"3.0"}
{"level":"info","ts":"2024-01-28T20:04:39.927Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.0"}
{"level":"info","ts":"2024-01-28T20:04:39.927Z","caller":"membership/cluster.go:576","msg":"updated cluster version","cluster-id":"b9ae902a9398fd61","local-member-id":"47324e080c98137d","from":"3.0","to":"3.5"}
{"level":"info","ts":"2024-01-28T20:04:39.927Z","caller":"rafthttp/peer_status.go:53","msg":"peer became active","peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:04:39.927Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:04:39.927Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:04:39.929Z","caller":"rafthttp/peer_status.go:53","msg":"peer became active","peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:04:39.929Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:04:39.929Z","caller":"rafthttp/stream.go:412","msg":"established TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:04:39.941Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"47324e080c98137d","to":"e2571e968b89c849","stream-type":"stream Message"}
{"level":"info","ts":"2024-01-28T20:04:39.941Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:04:39.941Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"47324e080c98137d","to":"e2571e968b89c849","stream-type":"stream MsgApp v2"}
{"level":"info","ts":"2024-01-28T20:04:39.942Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:04:39.964Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"47324e080c98137d [term: 6] received a MsgHeartbeat message with higher term from 39e3f227d2d33a6d [term: 7]"}
{"level":"info","ts":"2024-01-28T20:04:39.964Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"47324e080c98137d became follower at term 7"}
{"level":"info","ts":"2024-01-28T20:04:39.964Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 47324e080c98137d elected leader 39e3f227d2d33a6d at term 7"}
{"level":"info","ts":"2024-01-28T20:04:39.971Z","caller":"embed/serve.go:100","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-01-28T20:04:39.971Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"47324e080c98137d","local-member-attributes":"{Name:apisix-etcd-1 ClientURLs:[http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379 http://apisix-etcd.ingress-apisix.svc.cluster.local:2379]}","request-path":"/0/members/47324e080c98137d/attributes","cluster-id":"b9ae902a9398fd61","publish-timeout":"7s"}
{"level":"info","ts":"2024-01-28T20:04:39.971Z","caller":"etcdserver/server.go:777","msg":"initialized peer connections; fast-forwarding election ticks","local-member-id":"47324e080c98137d","forward-ticks":8,"forward-duration":"800ms","election-ticks":10,"election-timeout":"1s","active-remote-members":2}
{"level":"info","ts":"2024-01-28T20:04:39.971Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-01-28T20:04:39.971Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-01-28T20:04:39.972Z","caller":"embed/serve.go:146","msg":"serving client traffic insecurely; this is strongly discouraged!","address":"[::]:2379"}
{"level":"info","ts":"2024-01-28T20:04:40.002Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"47324e080c98137d","to":"39e3f227d2d33a6d","stream-type":"stream MsgApp v2"}
{"level":"info","ts":"2024-01-28T20:04:40.002Z","caller":"rafthttp/stream.go:249","msg":"set message encoder","from":"47324e080c98137d","to":"39e3f227d2d33a6d","stream-type":"stream Message"}
{"level":"info","ts":"2024-01-28T20:04:40.002Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:04:40.002Z","caller":"rafthttp/stream.go:274","msg":"established TCP streaming connection with remote peer","stream-writer-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"warn","ts":"2024-01-28T20:37:56.078Z","caller":"rafthttp/stream.go:421","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849","error":"EOF"}
{"level":"warn","ts":"2024-01-28T20:37:56.078Z","caller":"rafthttp/stream.go:421","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849","error":"EOF"}
{"level":"warn","ts":"2024-01-28T20:37:56.082Z","caller":"rafthttp/peer_status.go:66","msg":"peer became inactive (message send to peer failed)","peer-id":"e2571e968b89c849","error":"failed to dial e2571e968b89c849 on stream MsgApp v2 (read tcp 10.244.0.23:37762->10.244.0.20:2380: read: connection reset by peer)"}
{"level":"info","ts":"2024-01-28T20:37:56.089Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-01-28T20:37:56.089Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"apisix-etcd-1","data-dir":"/bitnami/etcd/data","advertise-peer-urls":["http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"],"advertise-client-urls":["http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379","http://apisix-etcd.ingress-apisix.svc.cluster.local:2379"]}
{"level":"info","ts":"2024-01-28T20:37:56.091Z","caller":"etcdserver/server.go:1456","msg":"skipped leadership transfer; local server is not leader","local-member-id":"47324e080c98137d","current-leader-member-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:37:56.091Z","caller":"rafthttp/peer.go:330","msg":"stopping remote peer","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"warn","ts":"2024-01-28T20:37:56.092Z","caller":"rafthttp/stream.go:286","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:37:56.092Z","caller":"rafthttp/stream.go:294","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"warn","ts":"2024-01-28T20:37:56.092Z","caller":"rafthttp/stream.go:286","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:37:56.092Z","caller":"rafthttp/stream.go:294","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:37:56.092Z","caller":"rafthttp/pipeline.go:85","msg":"stopped HTTP pipelining with remote peer","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"warn","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/stream.go:421","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d","error":"context canceled"}
{"level":"warn","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/peer_status.go:66","msg":"peer became inactive (message send to peer failed)","peer-id":"39e3f227d2d33a6d","error":"failed to read 39e3f227d2d33a6d on stream MsgApp v2 (context canceled)"}
{"level":"info","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/stream.go:442","msg":"stopped stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"warn","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/stream.go:421","msg":"lost TCP streaming connection with remote peer","stream-reader-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d","error":"context canceled"}
{"level":"info","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/stream.go:442","msg":"stopped stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/peer.go:335","msg":"stopped remote peer","remote-peer-id":"39e3f227d2d33a6d"}
{"level":"info","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/peer.go:330","msg":"stopping remote peer","remote-peer-id":"e2571e968b89c849"}
{"level":"warn","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/stream.go:286","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/stream.go:294","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream MsgApp v2","remote-peer-id":"e2571e968b89c849"}
{"level":"warn","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/stream.go:286","msg":"closed TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/stream.go:294","msg":"stopped TCP streaming connection with remote peer","stream-writer-type":"stream Message","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/pipeline.go:85","msg":"stopped HTTP pipelining with remote peer","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/stream.go:442","msg":"stopped stream reader with remote peer","stream-reader-type":"stream MsgApp v2","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.093Z","caller":"rafthttp/stream.go:442","msg":"stopped stream reader with remote peer","stream-reader-type":"stream Message","local-member-id":"47324e080c98137d","remote-peer-id":"e2571e968b89c849"}
{"level":"info","ts":"2024-01-28T20:37:56.094Z","caller":"rafthttp/peer.go:335","msg":"stopped remote peer","remote-peer-id":"e2571e968b89c849"}
{"level":"warn","ts":"2024-01-28T20:37:56.094Z","caller":"rafthttp/http.go:413","msg":"failed to find remote peer in cluster","local-member-id":"47324e080c98137d","remote-peer-id-stream-handler":"47324e080c98137d","remote-peer-id-from":"39e3f227d2d33a6d","cluster-id":"b9ae902a9398fd61"}
{"level":"warn","ts":"2024-01-28T20:37:56.094Z","caller":"rafthttp/http.go:413","msg":"failed to find remote peer in cluster","local-member-id":"47324e080c98137d","remote-peer-id-stream-handler":"47324e080c98137d","remote-peer-id-from":"39e3f227d2d33a6d","cluster-id":"b9ae902a9398fd61"}
{"level":"info","ts":"2024-01-28T20:37:56.098Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"[::]:2380"}
{"level":"info","ts":"2024-01-28T20:37:56.112Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"[::]:2380"}
{"level":"info","ts":"2024-01-28T20:37:56.112Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"apisix-etcd-1","data-dir":"/bitnami/etcd/data","advertise-peer-urls":["http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2380"],"advertise-client-urls":["http://apisix-etcd-1.apisix-etcd-headless.ingress-apisix.svc.cluster.local:2379","http://apisix-etcd.ingress-apisix.svc.cluster.local:2379"]}

* 
* ==> kernel <==
*  20:42:29 up  5:49,  0 users,  load average: 0.65, 1.41, 1.41
Linux minikube 6.1.0-16-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.67-1 (2023-12-12) x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [7a8017edeef2] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 20:38:05.911578       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 20:38:05.923626       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 20:38:05.997443       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 20:38:06.003188       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 20:38:06.006874       1 logging.go:59] [core] [Channel #16 SubChannel #17] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 20:38:06.028446       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 20:38:06.078275       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-apiserver [a0515ef69884] <==
* I0128 20:38:14.815790       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0128 20:38:14.815923       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0128 20:38:14.815991       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0128 20:38:14.816162       1 controller.go:121] Starting legacy_token_tracking_controller
I0128 20:38:14.816177       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0128 20:38:14.816173       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0128 20:38:14.816368       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0128 20:38:14.816382       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0128 20:38:14.816518       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0128 20:38:14.815710       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0128 20:38:14.815724       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0128 20:38:14.815617       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0128 20:38:14.818021       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0128 20:38:14.818795       1 controller.go:85] Starting OpenAPI controller
I0128 20:38:14.818993       1 controller.go:85] Starting OpenAPI V3 controller
I0128 20:38:14.819156       1 naming_controller.go:291] Starting NamingConditionController
I0128 20:38:14.819309       1 establishing_controller.go:76] Starting EstablishingController
I0128 20:38:14.819454       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0128 20:38:14.819476       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0128 20:38:14.819512       1 crd_finalizer.go:266] Starting CRDFinalizer
E0128 20:38:14.862408       1 controller.go:155] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0128 20:38:14.882175       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0128 20:38:14.915402       1 apf_controller.go:366] Running API Priority and Fairness config worker
I0128 20:38:14.915417       1 apf_controller.go:369] Running API Priority and Fairness periodic rebalancing process
I0128 20:38:14.916074       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0128 20:38:14.916095       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0128 20:38:14.916224       1 aggregator.go:152] initial CRD sync complete...
I0128 20:38:14.916240       1 autoregister_controller.go:141] Starting autoregister controller
I0128 20:38:14.916250       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0128 20:38:14.916260       1 cache.go:39] Caches are synced for autoregister controller
I0128 20:38:14.916228       1 shared_informer.go:318] Caches are synced for configmaps
I0128 20:38:14.916415       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0128 20:38:14.917587       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0128 20:38:14.918370       1 handler.go:232] Adding GroupVersion apisix.apache.org v2beta3 to ResourceManager
I0128 20:38:14.918439       1 handler.go:232] Adding GroupVersion apisix.apache.org v2 to ResourceManager
I0128 20:38:14.918504       1 handler.go:232] Adding GroupVersion apisix.apache.org v2beta2 to ResourceManager
I0128 20:38:14.918555       1 handler.go:232] Adding GroupVersion apisix.apache.org v2beta3 to ResourceManager
I0128 20:38:14.918661       1 handler.go:232] Adding GroupVersion apisix.apache.org v2 to ResourceManager
I0128 20:38:14.918752       1 handler.go:232] Adding GroupVersion apisix.apache.org v2beta3 to ResourceManager
I0128 20:38:14.918836       1 handler.go:232] Adding GroupVersion apisix.apache.org v2 to ResourceManager
I0128 20:38:14.918935       1 handler.go:232] Adding GroupVersion apisix.apache.org v2beta3 to ResourceManager
I0128 20:38:14.919032       1 handler.go:232] Adding GroupVersion apisix.apache.org v2 to ResourceManager
I0128 20:38:14.919125       1 handler.go:232] Adding GroupVersion apisix.apache.org v2beta3 to ResourceManager
I0128 20:38:14.919203       1 handler.go:232] Adding GroupVersion apisix.apache.org v2 to ResourceManager
I0128 20:38:14.919293       1 handler.go:232] Adding GroupVersion apisix.apache.org v2beta3 to ResourceManager
I0128 20:38:14.919381       1 handler.go:232] Adding GroupVersion apisix.apache.org v2 to ResourceManager
I0128 20:38:14.919545       1 handler.go:232] Adding GroupVersion apisix.apache.org v2beta3 to ResourceManager
I0128 20:38:14.919692       1 handler.go:232] Adding GroupVersion apisix.apache.org v2 to ResourceManager
I0128 20:38:14.919810       1 handler.go:232] Adding GroupVersion apisix.apache.org v2beta2 to ResourceManager
I0128 20:38:14.976724       1 shared_informer.go:318] Caches are synced for node_authorizer
I0128 20:38:15.522042       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0128 20:38:15.822922       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0128 20:38:27.084944       1 controller.go:624] quota admission added evaluator for: endpoints
I0128 20:38:27.084946       1 controller.go:624] quota admission added evaluator for: endpoints
I0128 20:38:27.095267       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0128 20:38:27.095267       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0128 20:38:27.095267       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0128 20:41:08.453123       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0128 20:41:08.458643       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0128 20:41:16.227924       1 alloc.go:330] "allocated clusterIPs" service="default/ner-extractor-svc" clusterIPs=map[IPv4:10.96.24.115]

* 
* ==> kube-controller-manager [b2bb4a55c13d] <==
* I0128 20:38:27.030527       1 controllermanager.go:638] "Started controller" controller="statefulset"
I0128 20:38:27.030650       1 stateful_set.go:161] "Starting stateful set controller"
I0128 20:38:27.030680       1 shared_informer.go:311] Waiting for caches to sync for stateful set
I0128 20:38:27.034869       1 controllermanager.go:638] "Started controller" controller="attachdetach"
I0128 20:38:27.035086       1 attach_detach_controller.go:343] "Starting attach detach controller"
I0128 20:38:27.035119       1 shared_informer.go:311] Waiting for caches to sync for attach detach
I0128 20:38:27.044992       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0128 20:38:27.064696       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0128 20:38:27.066150       1 shared_informer.go:318] Caches are synced for node
I0128 20:38:27.066200       1 range_allocator.go:174] "Sending events to api server"
I0128 20:38:27.066230       1 range_allocator.go:178] "Starting range CIDR allocator"
I0128 20:38:27.066239       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0128 20:38:27.066252       1 shared_informer.go:318] Caches are synced for cidrallocator
I0128 20:38:27.066608       1 shared_informer.go:318] Caches are synced for TTL after finished
I0128 20:38:27.068711       1 shared_informer.go:318] Caches are synced for taint
I0128 20:38:27.068778       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0128 20:38:27.068798       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0128 20:38:27.068909       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0128 20:38:27.068950       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0128 20:38:27.069004       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0128 20:38:27.069008       1 taint_manager.go:211] "Sending events to api server"
I0128 20:38:27.069141       1 shared_informer.go:318] Caches are synced for endpoint
I0128 20:38:27.069536       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0128 20:38:27.070998       1 shared_informer.go:318] Caches are synced for expand
I0128 20:38:27.073941       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0128 20:38:27.079729       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0128 20:38:27.079890       1 shared_informer.go:318] Caches are synced for HPA
I0128 20:38:27.081946       1 shared_informer.go:318] Caches are synced for cronjob
I0128 20:38:27.088417       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0128 20:38:27.090243       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0128 20:38:27.090564       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0128 20:38:27.092031       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0128 20:38:27.094868       1 shared_informer.go:318] Caches are synced for crt configmap
I0128 20:38:27.097255       1 shared_informer.go:318] Caches are synced for ReplicationController
I0128 20:38:27.099351       1 shared_informer.go:318] Caches are synced for deployment
I0128 20:38:27.100614       1 shared_informer.go:318] Caches are synced for TTL
I0128 20:38:27.103453       1 shared_informer.go:318] Caches are synced for daemon sets
I0128 20:38:27.107985       1 shared_informer.go:318] Caches are synced for ephemeral
I0128 20:38:27.112129       1 shared_informer.go:318] Caches are synced for persistent volume
I0128 20:38:27.112244       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0128 20:38:27.115491       1 shared_informer.go:318] Caches are synced for PVC protection
I0128 20:38:27.117828       1 shared_informer.go:318] Caches are synced for PV protection
I0128 20:38:27.120128       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0128 20:38:27.127805       1 shared_informer.go:318] Caches are synced for job
I0128 20:38:27.131104       1 shared_informer.go:318] Caches are synced for stateful set
I0128 20:38:27.139288       1 shared_informer.go:318] Caches are synced for namespace
I0128 20:38:27.142509       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0128 20:38:27.145078       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0128 20:38:27.147368       1 shared_informer.go:318] Caches are synced for GC
I0128 20:38:27.149713       1 shared_informer.go:318] Caches are synced for service account
I0128 20:38:27.153820       1 shared_informer.go:318] Caches are synced for disruption
I0128 20:38:27.235526       1 shared_informer.go:318] Caches are synced for attach detach
I0128 20:38:27.252158       1 shared_informer.go:318] Caches are synced for resource quota
I0128 20:38:27.305276       1 shared_informer.go:318] Caches are synced for resource quota
I0128 20:38:27.670322       1 shared_informer.go:318] Caches are synced for garbage collector
I0128 20:38:27.722435       1 shared_informer.go:318] Caches are synced for garbage collector
I0128 20:38:27.722484       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0128 20:41:08.461572       1 event.go:307] "Event occurred" object="default/ner-extractor-svc" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ner-extractor-svc-7d454cd9bd to 2"
I0128 20:41:08.468650       1 event.go:307] "Event occurred" object="default/ner-extractor-svc-7d454cd9bd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ner-extractor-svc-7d454cd9bd-lr2rm"
I0128 20:41:08.473991       1 event.go:307] "Event occurred" object="default/ner-extractor-svc-7d454cd9bd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ner-extractor-svc-7d454cd9bd-l6np9"

* 
* ==> kube-controller-manager [edd74f96dec5] <==
* I0128 20:03:33.137618       1 daemon_controller.go:291] "Starting daemon sets controller"
I0128 20:03:33.137636       1 shared_informer.go:311] Waiting for caches to sync for daemon sets
I0128 20:03:33.143132       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I0128 20:03:33.148804       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0128 20:03:33.151512       1 shared_informer.go:318] Caches are synced for job
I0128 20:03:33.152424       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0128 20:03:33.156660       1 shared_informer.go:318] Caches are synced for persistent volume
I0128 20:03:33.157338       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0128 20:03:33.159380       1 shared_informer.go:318] Caches are synced for PVC protection
I0128 20:03:33.160986       1 shared_informer.go:318] Caches are synced for taint
I0128 20:03:33.161038       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0128 20:03:33.161086       1 taint_manager.go:211] "Sending events to api server"
I0128 20:03:33.161287       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0128 20:03:33.161271       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I0128 20:03:33.161905       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0128 20:03:33.161971       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I0128 20:03:33.165583       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0128 20:03:33.166788       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I0128 20:03:33.171118       1 shared_informer.go:318] Caches are synced for PV protection
I0128 20:03:33.173119       1 shared_informer.go:318] Caches are synced for TTL after finished
I0128 20:03:33.182591       1 shared_informer.go:318] Caches are synced for deployment
I0128 20:03:33.184935       1 shared_informer.go:318] Caches are synced for endpoint
I0128 20:03:33.187772       1 shared_informer.go:318] Caches are synced for ReplicationController
I0128 20:03:33.190994       1 shared_informer.go:318] Caches are synced for service account
I0128 20:03:33.193300       1 shared_informer.go:318] Caches are synced for namespace
I0128 20:03:33.199114       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0128 20:03:33.201306       1 shared_informer.go:318] Caches are synced for attach detach
I0128 20:03:33.201355       1 shared_informer.go:318] Caches are synced for ephemeral
I0128 20:03:33.203522       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0128 20:03:33.203617       1 shared_informer.go:318] Caches are synced for expand
I0128 20:03:33.206043       1 shared_informer.go:318] Caches are synced for GC
I0128 20:03:33.207169       1 shared_informer.go:318] Caches are synced for TTL
I0128 20:03:33.209345       1 shared_informer.go:318] Caches are synced for crt configmap
I0128 20:03:33.218769       1 shared_informer.go:318] Caches are synced for HPA
I0128 20:03:33.220989       1 shared_informer.go:318] Caches are synced for cronjob
I0128 20:03:33.226262       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0128 20:03:33.227427       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0128 20:03:33.229651       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0128 20:03:33.230797       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0128 20:03:33.238202       1 shared_informer.go:318] Caches are synced for daemon sets
I0128 20:03:33.245704       1 shared_informer.go:318] Caches are synced for node
I0128 20:03:33.245757       1 range_allocator.go:174] "Sending events to api server"
I0128 20:03:33.245785       1 range_allocator.go:178] "Starting range CIDR allocator"
I0128 20:03:33.245791       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0128 20:03:33.245798       1 shared_informer.go:318] Caches are synced for cidrallocator
I0128 20:03:33.247963       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0128 20:03:33.364174       1 shared_informer.go:318] Caches are synced for disruption
I0128 20:03:33.396822       1 shared_informer.go:318] Caches are synced for resource quota
I0128 20:03:33.396922       1 shared_informer.go:318] Caches are synced for stateful set
I0128 20:03:33.443644       1 shared_informer.go:318] Caches are synced for resource quota
I0128 20:03:33.766636       1 shared_informer.go:318] Caches are synced for garbage collector
I0128 20:03:33.779848       1 shared_informer.go:318] Caches are synced for garbage collector
I0128 20:03:33.779885       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0128 20:04:59.665138       1 event.go:307] "Event occurred" object="default/ner-extractor-svc" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ner-extractor-svc-79466d9b7c to 1"
I0128 20:04:59.673848       1 event.go:307] "Event occurred" object="default/ner-extractor-svc-79466d9b7c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ner-extractor-svc-79466d9b7c-bfn6p"
I0128 20:07:32.598655       1 event.go:307] "Event occurred" object="default/ner-extractor-svc" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ner-extractor-svc-6b679b579c to 1"
I0128 20:07:32.605563       1 event.go:307] "Event occurred" object="default/ner-extractor-svc-6b679b579c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ner-extractor-svc-6b679b579c-99rt2"
I0128 20:28:46.113355       1 event.go:307] "Event occurred" object="default/ner-extractor-svc" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ner-extractor-svc-7d454cd9bd to 2"
I0128 20:28:46.120017       1 event.go:307] "Event occurred" object="default/ner-extractor-svc-7d454cd9bd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ner-extractor-svc-7d454cd9bd-htp4g"
I0128 20:28:46.125461       1 event.go:307] "Event occurred" object="default/ner-extractor-svc-7d454cd9bd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ner-extractor-svc-7d454cd9bd-g7d4h"

* 
* ==> kube-proxy [8d9bf3b02acf] <==
* I0128 20:03:22.462980       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0128 20:03:22.463092       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0128 20:03:22.463164       1 server_others.go:554] "Using iptables proxy"
I0128 20:03:22.490623       1 server_others.go:192] "Using iptables Proxier"
I0128 20:03:22.490659       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0128 20:03:22.490676       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0128 20:03:22.490706       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0128 20:03:22.490754       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0128 20:03:22.491699       1 server.go:658] "Version info" version="v1.27.4"
I0128 20:03:22.491719       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0128 20:03:22.493034       1 config.go:97] "Starting endpoint slice config controller"
I0128 20:03:22.493085       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0128 20:03:22.493132       1 config.go:315] "Starting node config controller"
I0128 20:03:22.493157       1 shared_informer.go:311] Waiting for caches to sync for node config
I0128 20:03:22.493095       1 config.go:188] "Starting service config controller"
I0128 20:03:22.494154       1 shared_informer.go:311] Waiting for caches to sync for service config
I0128 20:03:22.593872       1 shared_informer.go:318] Caches are synced for node config
I0128 20:03:22.593942       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0128 20:03:22.594969       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-proxy [a5a793beef7d] <==
* I0128 20:38:14.888306       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0128 20:38:14.888404       1 server_others.go:110] "Detected node IP" address="192.168.49.2"
I0128 20:38:14.888443       1 server_others.go:554] "Using iptables proxy"
I0128 20:38:14.918096       1 server_others.go:192] "Using iptables Proxier"
I0128 20:38:14.918141       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0128 20:38:14.918164       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0128 20:38:14.918189       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0128 20:38:14.918242       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0128 20:38:14.919466       1 server.go:658] "Version info" version="v1.27.4"
I0128 20:38:14.919631       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0128 20:38:14.920570       1 config.go:97] "Starting endpoint slice config controller"
I0128 20:38:14.920635       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0128 20:38:14.920739       1 config.go:188] "Starting service config controller"
I0128 20:38:14.920758       1 shared_informer.go:311] Waiting for caches to sync for service config
I0128 20:38:14.920599       1 config.go:315] "Starting node config controller"
I0128 20:38:14.920792       1 shared_informer.go:311] Waiting for caches to sync for node config
I0128 20:38:15.021826       1 shared_informer.go:318] Caches are synced for node config
I0128 20:38:15.021853       1 shared_informer.go:318] Caches are synced for service config
I0128 20:38:15.021866       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [4df0cd77598a] <==
* I0128 20:38:12.861965       1 serving.go:348] Generated self-signed cert in-memory
W0128 20:38:14.856689       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0128 20:38:14.856922       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0128 20:38:14.857011       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0128 20:38:14.857124       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0128 20:38:14.885135       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I0128 20:38:14.885182       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0128 20:38:14.888701       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0128 20:38:14.888737       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0128 20:38:14.890161       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0128 20:38:14.890243       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0128 20:38:14.992166       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [b090209ba3bc] <==
* I0128 20:03:18.712995       1 serving.go:348] Generated self-signed cert in-memory
W0128 20:03:20.864634       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0128 20:03:20.864673       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0128 20:03:20.864693       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0128 20:03:20.864708       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0128 20:03:20.885836       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.27.4"
I0128 20:03:20.885859       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0128 20:03:20.888023       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0128 20:03:20.888091       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0128 20:03:20.889197       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0128 20:03:20.889237       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0128 20:03:20.988342       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0128 20:37:56.082843       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I0128 20:37:56.082918       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0128 20:37:56.083219       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0128 20:37:56.083265       1 scheduling_queue.go:1139] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
E0128 20:37:56.083343       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Jan 28 20:41:48 minikube kubelet[1859]: E0128 20:41:48.852785    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-proxy-vtfrm_c0fe3336-d90a-4241-b964-4d3014ef7b4a/kube-proxy/2.log\": no such file or directory" containerName="kube-proxy"
Jan 28 20:41:48 minikube kubelet[1859]: E0128 20:41:48.852871    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-controller-manager-minikube_b3702ceb912504d37098b922ccdcfa41/kube-controller-manager/2.log\": no such file or directory" containerName="kube-controller-manager"
Jan 28 20:41:48 minikube kubelet[1859]: E0128 20:41:48.852939    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_storage-provisioner_dafca340-9fc1-4fcf-8962-c552e72b3d80/storage-provisioner/5.log\": no such file or directory" containerName="storage-provisioner"
Jan 28 20:41:48 minikube kubelet[1859]: E0128 20:41:48.853032    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-apiserver-minikube_f241819aff4d77a34fc71bea1fac9af8/kube-apiserver/2.log\": no such file or directory" containerName="kube-apiserver"
Jan 28 20:41:48 minikube kubelet[1859]: E0128 20:41:48.853094    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-etcd-0_69c8468b-2071-40ed-9358-05e819a4c4d4/etcd/1.log\": no such file or directory" containerName="etcd"
Jan 28 20:41:48 minikube kubelet[1859]: E0128 20:41:48.853154    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-etcd-1_2d94c07c-fd47-4187-b776-f95442b4d26c/etcd/1.log\": no such file or directory" containerName="etcd"
Jan 28 20:41:48 minikube kubelet[1859]: E0128 20:41:48.853245    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-etcd-2_2860f36d-bda7-4ffc-8725-afb498579772/etcd/1.log\": no such file or directory" containerName="etcd"
Jan 28 20:41:48 minikube kubelet[1859]: E0128 20:41:48.853306    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_etcd-minikube_8af0e85a28544808d52bb7c47ad824ed/etcd/2.log\": no such file or directory" containerName="etcd"
Jan 28 20:41:48 minikube kubelet[1859]: E0128 20:41:48.853400    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-ingress-controller-78ff6d8549-jk7v8_5f9a21cc-d127-4cc5-ae34-ef87450fcd01/ingress-controller/0.log\": no such file or directory" containerName="ingress-controller"
Jan 28 20:41:48 minikube kubelet[1859]: E0128 20:41:48.853465    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-bf7895f94-q4mlp_3e961e23-0f2f-46db-af27-f18adf3c5c60/apisix/0.log\": no such file or directory" containerName="apisix"
Jan 28 20:41:48 minikube kubelet[1859]: E0128 20:41:48.853524    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-scheduler-minikube_eb675835e10503c79265cf0e2983f93c/kube-scheduler/2.log\": no such file or directory" containerName="kube-scheduler"
Jan 28 20:41:48 minikube kubelet[1859]: E0128 20:41:48.853603    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_coredns-5d78c9869d-49fn2_70880a1f-3d79-4869-9172-25ecab22167d/coredns/3.log\": no such file or directory" containerName="coredns"
Jan 28 20:41:58 minikube kubelet[1859]: E0128 20:41:58.520763    1859 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for ner_extractor_svc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="ner_extractor_svc:latest"
Jan 28 20:41:58 minikube kubelet[1859]: E0128 20:41:58.520814    1859 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for ner_extractor_svc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="ner_extractor_svc:latest"
Jan 28 20:41:58 minikube kubelet[1859]: E0128 20:41:58.520979    1859 kuberuntime_manager.go:1212] container &Container{Name:ner-extractor,Image:ner_extractor_svc:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f7wnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod ner-extractor-svc-7d454cd9bd-lr2rm_default(f702c7c4-8089-4ebd-b603-413ea7c4bbbd): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for ner_extractor_svc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jan 28 20:41:58 minikube kubelet[1859]: E0128 20:41:58.521051    1859 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ner-extractor\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for ner_extractor_svc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/ner-extractor-svc-7d454cd9bd-lr2rm" podUID=f702c7c4-8089-4ebd-b603-413ea7c4bbbd
Jan 28 20:42:00 minikube kubelet[1859]: E0128 20:42:00.020061    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-scheduler-minikube_eb675835e10503c79265cf0e2983f93c/kube-scheduler/2.log\": no such file or directory" containerName="kube-scheduler"
Jan 28 20:42:00 minikube kubelet[1859]: E0128 20:42:00.020151    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-controller-manager-minikube_b3702ceb912504d37098b922ccdcfa41/kube-controller-manager/2.log\": no such file or directory" containerName="kube-controller-manager"
Jan 28 20:42:00 minikube kubelet[1859]: E0128 20:42:00.020217    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_etcd-minikube_8af0e85a28544808d52bb7c47ad824ed/etcd/2.log\": no such file or directory" containerName="etcd"
Jan 28 20:42:00 minikube kubelet[1859]: E0128 20:42:00.020278    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-apiserver-minikube_f241819aff4d77a34fc71bea1fac9af8/kube-apiserver/2.log\": no such file or directory" containerName="kube-apiserver"
Jan 28 20:42:00 minikube kubelet[1859]: E0128 20:42:00.020336    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-etcd-1_2d94c07c-fd47-4187-b776-f95442b4d26c/etcd/1.log\": no such file or directory" containerName="etcd"
Jan 28 20:42:00 minikube kubelet[1859]: E0128 20:42:00.020398    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_coredns-5d78c9869d-49fn2_70880a1f-3d79-4869-9172-25ecab22167d/coredns/3.log\": no such file or directory" containerName="coredns"
Jan 28 20:42:00 minikube kubelet[1859]: E0128 20:42:00.020493    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-etcd-0_69c8468b-2071-40ed-9358-05e819a4c4d4/etcd/1.log\": no such file or directory" containerName="etcd"
Jan 28 20:42:00 minikube kubelet[1859]: E0128 20:42:00.020557    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-ingress-controller-78ff6d8549-jk7v8_5f9a21cc-d127-4cc5-ae34-ef87450fcd01/ingress-controller/0.log\": no such file or directory" containerName="ingress-controller"
Jan 28 20:42:00 minikube kubelet[1859]: E0128 20:42:00.020637    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-proxy-vtfrm_c0fe3336-d90a-4241-b964-4d3014ef7b4a/kube-proxy/2.log\": no such file or directory" containerName="kube-proxy"
Jan 28 20:42:00 minikube kubelet[1859]: E0128 20:42:00.020708    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-etcd-2_2860f36d-bda7-4ffc-8725-afb498579772/etcd/1.log\": no such file or directory" containerName="etcd"
Jan 28 20:42:00 minikube kubelet[1859]: E0128 20:42:00.020770    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-bf7895f94-q4mlp_3e961e23-0f2f-46db-af27-f18adf3c5c60/apisix/0.log\": no such file or directory" containerName="apisix"
Jan 28 20:42:00 minikube kubelet[1859]: E0128 20:42:00.020851    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_storage-provisioner_dafca340-9fc1-4fcf-8962-c552e72b3d80/storage-provisioner/5.log\": no such file or directory" containerName="storage-provisioner"
Jan 28 20:42:01 minikube kubelet[1859]: E0128 20:42:01.184397    1859 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for ner_extractor_svc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="ner_extractor_svc:latest"
Jan 28 20:42:01 minikube kubelet[1859]: E0128 20:42:01.184489    1859 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for ner_extractor_svc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="ner_extractor_svc:latest"
Jan 28 20:42:01 minikube kubelet[1859]: E0128 20:42:01.184650    1859 kuberuntime_manager.go:1212] container &Container{Name:ner-extractor,Image:ner_extractor_svc:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cfbvt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod ner-extractor-svc-7d454cd9bd-l6np9_default(c747bef4-e2c1-454a-9ea2-0400e55b5313): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for ner_extractor_svc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Jan 28 20:42:01 minikube kubelet[1859]: E0128 20:42:01.184744    1859 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ner-extractor\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for ner_extractor_svc, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/ner-extractor-svc-7d454cd9bd-l6np9" podUID=c747bef4-e2c1-454a-9ea2-0400e55b5313
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.215737    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-ingress-controller-78ff6d8549-jk7v8_5f9a21cc-d127-4cc5-ae34-ef87450fcd01/ingress-controller/0.log\": no such file or directory" containerName="ingress-controller"
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.215840    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-etcd-0_69c8468b-2071-40ed-9358-05e819a4c4d4/etcd/1.log\": no such file or directory" containerName="etcd"
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.215962    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-scheduler-minikube_eb675835e10503c79265cf0e2983f93c/kube-scheduler/2.log\": no such file or directory" containerName="kube-scheduler"
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.216057    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-apiserver-minikube_f241819aff4d77a34fc71bea1fac9af8/kube-apiserver/2.log\": no such file or directory" containerName="kube-apiserver"
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.216143    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-bf7895f94-q4mlp_3e961e23-0f2f-46db-af27-f18adf3c5c60/apisix/0.log\": no such file or directory" containerName="apisix"
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.216221    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_coredns-5d78c9869d-49fn2_70880a1f-3d79-4869-9172-25ecab22167d/coredns/3.log\": no such file or directory" containerName="coredns"
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.216298    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-etcd-1_2d94c07c-fd47-4187-b776-f95442b4d26c/etcd/1.log\": no such file or directory" containerName="etcd"
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.216375    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-proxy-vtfrm_c0fe3336-d90a-4241-b964-4d3014ef7b4a/kube-proxy/2.log\": no such file or directory" containerName="kube-proxy"
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.216457    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_storage-provisioner_dafca340-9fc1-4fcf-8962-c552e72b3d80/storage-provisioner/5.log\": no such file or directory" containerName="storage-provisioner"
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.216532    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_etcd-minikube_8af0e85a28544808d52bb7c47ad824ed/etcd/2.log\": no such file or directory" containerName="etcd"
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.216644    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-controller-manager-minikube_b3702ceb912504d37098b922ccdcfa41/kube-controller-manager/2.log\": no such file or directory" containerName="kube-controller-manager"
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.216723    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-etcd-2_2860f36d-bda7-4ffc-8725-afb498579772/etcd/1.log\": no such file or directory" containerName="etcd"
Jan 28 20:42:11 minikube kubelet[1859]: E0128 20:42:11.759396    1859 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ner-extractor\" with ImagePullBackOff: \"Back-off pulling image \\\"ner_extractor_svc:latest\\\"\"" pod="default/ner-extractor-svc-7d454cd9bd-lr2rm" podUID=f702c7c4-8089-4ebd-b603-413ea7c4bbbd
Jan 28 20:42:16 minikube kubelet[1859]: E0128 20:42:16.759992    1859 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ner-extractor\" with ImagePullBackOff: \"Back-off pulling image \\\"ner_extractor_svc:latest\\\"\"" pod="default/ner-extractor-svc-7d454cd9bd-l6np9" podUID=c747bef4-e2c1-454a-9ea2-0400e55b5313
Jan 28 20:42:22 minikube kubelet[1859]: E0128 20:42:22.426339    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_storage-provisioner_dafca340-9fc1-4fcf-8962-c552e72b3d80/storage-provisioner/5.log\": no such file or directory" containerName="storage-provisioner"
Jan 28 20:42:22 minikube kubelet[1859]: E0128 20:42:22.426485    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-bf7895f94-q4mlp_3e961e23-0f2f-46db-af27-f18adf3c5c60/apisix/0.log\": no such file or directory" containerName="apisix"
Jan 28 20:42:22 minikube kubelet[1859]: E0128 20:42:22.426595    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-etcd-1_2d94c07c-fd47-4187-b776-f95442b4d26c/etcd/1.log\": no such file or directory" containerName="etcd"
Jan 28 20:42:22 minikube kubelet[1859]: E0128 20:42:22.426686    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-controller-manager-minikube_b3702ceb912504d37098b922ccdcfa41/kube-controller-manager/2.log\": no such file or directory" containerName="kube-controller-manager"
Jan 28 20:42:22 minikube kubelet[1859]: E0128 20:42:22.426774    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-scheduler-minikube_eb675835e10503c79265cf0e2983f93c/kube-scheduler/2.log\": no such file or directory" containerName="kube-scheduler"
Jan 28 20:42:22 minikube kubelet[1859]: E0128 20:42:22.426860    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-proxy-vtfrm_c0fe3336-d90a-4241-b964-4d3014ef7b4a/kube-proxy/2.log\": no such file or directory" containerName="kube-proxy"
Jan 28 20:42:22 minikube kubelet[1859]: E0128 20:42:22.426951    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-ingress-controller-78ff6d8549-jk7v8_5f9a21cc-d127-4cc5-ae34-ef87450fcd01/ingress-controller/0.log\": no such file or directory" containerName="ingress-controller"
Jan 28 20:42:22 minikube kubelet[1859]: E0128 20:42:22.427079    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_etcd-minikube_8af0e85a28544808d52bb7c47ad824ed/etcd/2.log\": no such file or directory" containerName="etcd"
Jan 28 20:42:22 minikube kubelet[1859]: E0128 20:42:22.427207    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-etcd-0_69c8468b-2071-40ed-9358-05e819a4c4d4/etcd/1.log\": no such file or directory" containerName="etcd"
Jan 28 20:42:22 minikube kubelet[1859]: E0128 20:42:22.427293    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/ingress-apisix_apisix-etcd-2_2860f36d-bda7-4ffc-8725-afb498579772/etcd/1.log\": no such file or directory" containerName="etcd"
Jan 28 20:42:22 minikube kubelet[1859]: E0128 20:42:22.427377    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_coredns-5d78c9869d-49fn2_70880a1f-3d79-4869-9172-25ecab22167d/coredns/3.log\": no such file or directory" containerName="coredns"
Jan 28 20:42:22 minikube kubelet[1859]: E0128 20:42:22.427462    1859 cri_stats_provider.go:643] "Unable to fetch container log stats" err="failed to get fsstats for \"/var/log/pods/kube-system_kube-apiserver-minikube_f241819aff4d77a34fc71bea1fac9af8/kube-apiserver/2.log\": no such file or directory" containerName="kube-apiserver"
Jan 28 20:42:26 minikube kubelet[1859]: E0128 20:42:26.759671    1859 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ner-extractor\" with ImagePullBackOff: \"Back-off pulling image \\\"ner_extractor_svc:latest\\\"\"" pod="default/ner-extractor-svc-7d454cd9bd-lr2rm" podUID=f702c7c4-8089-4ebd-b603-413ea7c4bbbd
Jan 28 20:42:28 minikube kubelet[1859]: E0128 20:42:28.757171    1859 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ner-extractor\" with ImagePullBackOff: \"Back-off pulling image \\\"ner_extractor_svc:latest\\\"\"" pod="default/ner-extractor-svc-7d454cd9bd-l6np9" podUID=c747bef4-e2c1-454a-9ea2-0400e55b5313

* 
* ==> storage-provisioner [b8b40f8ea88d] <==
* I0128 20:38:28.929346       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0128 20:38:28.939480       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0128 20:38:28.939522       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0128 20:38:46.363055       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0128 20:38:46.363400       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e7bccab7-438c-43c1-909b-19637d406265!
I0128 20:38:46.363354       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"705a2947-91e3-47d4-9a4c-d41b7475d132", APIVersion:"v1", ResourceVersion:"327906", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e7bccab7-438c-43c1-909b-19637d406265 became leader
I0128 20:38:46.463820       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e7bccab7-438c-43c1-909b-19637d406265!

* 
* ==> storage-provisioner [c71dd39b877b] <==
* I0128 20:38:11.810104       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0128 20:38:11.813148       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

